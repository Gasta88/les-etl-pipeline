{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main ETL for Loan Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, StringType, DoubleType, BooleanType, TimestampType, IntegerType\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "SPARK = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "BRONZE_SOURCE_DIR = \"../data/mini_source\"\n",
    "SILVER_SOURCE_DIR = \"../data/output/bronze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSET SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\n",
    "        \"AS1\",\n",
    "        \"AS19\",\n",
    "        \"AS20\",\n",
    "        \"AS31\",\n",
    "        \"AS50\",\n",
    "        \"AS51\",\n",
    "        \"AS67\",\n",
    "        \"AS70\",\n",
    "        \"AS71\",\n",
    "        \"AS87\",\n",
    "        \"AS91\",\n",
    "        \"AS112\",\n",
    "        \"AS124\",\n",
    "        \"AS127\",\n",
    "        \"AS130\",\n",
    "        \"AS133\",\n",
    "        \"AS134\",\n",
    "        \"AS137\",\n",
    "    ]\n",
    "    config[\"ASSET_COLUMNS\"] = {\n",
    "        \"AS1\": DateType(),\n",
    "        \"AS2\": StringType(),\n",
    "        \"AS3\": StringType(),\n",
    "        \"AS4\": StringType(),\n",
    "        \"AS5\": StringType(),\n",
    "        \"AS6\": StringType(),\n",
    "        \"AS7\": StringType(),\n",
    "        \"AS8\": StringType(),\n",
    "        \"AS15\": StringType(),\n",
    "        \"AS16\": StringType(),\n",
    "        \"AS17\": StringType(),\n",
    "        \"AS18\": StringType(),\n",
    "        \"AS19\": DateType(),\n",
    "        \"AS20\": DateType(),\n",
    "        \"AS21\": StringType(),\n",
    "        \"AS22\": StringType(),\n",
    "        \"AS23\": BooleanType(),\n",
    "        \"AS24\": StringType(),\n",
    "        \"AS25\": StringType(),\n",
    "        \"AS26\": StringType(),\n",
    "        \"AS27\": DoubleType(),\n",
    "        \"AS28\": DoubleType(),\n",
    "        \"AS29\": BooleanType(),\n",
    "        \"AS30\": DoubleType(),\n",
    "        \"AS31\": DateType(),\n",
    "        \"AS32\": StringType(),\n",
    "        \"AS33\": StringType(),\n",
    "        \"AS34\": StringType(),\n",
    "        \"AS35\": StringType(),\n",
    "        \"AS36\": StringType(),\n",
    "        \"AS37\": DoubleType(),\n",
    "        \"AS38\": DoubleType(),\n",
    "        \"AS39\": DoubleType(),\n",
    "        \"AS40\": DoubleType(),\n",
    "        \"AS41\": DoubleType(),\n",
    "        \"AS42\": StringType(),\n",
    "        \"AS43\": StringType(),\n",
    "        \"AS44\": DoubleType(),\n",
    "        \"AS45\": StringType(),\n",
    "        \"AS50\": DateType(),\n",
    "        \"AS51\": DateType(),\n",
    "        \"AS52\": StringType(),\n",
    "        \"AS53\": BooleanType(),\n",
    "        \"AS54\": DoubleType(),\n",
    "        \"AS55\": DoubleType(),\n",
    "        \"AS56\": DoubleType(),\n",
    "        \"AS57\": StringType(),\n",
    "        \"AS58\": StringType(),\n",
    "        \"AS59\": StringType(),\n",
    "        \"AS60\": DoubleType(),\n",
    "        \"AS61\": DoubleType(),\n",
    "        \"AS62\": StringType(),\n",
    "        \"AS63\": DoubleType(),\n",
    "        \"AS64\": DoubleType(),\n",
    "        \"AS65\": StringType(),\n",
    "        \"AS66\": DoubleType(),\n",
    "        \"AS67\": DateType(),\n",
    "        \"AS68\": StringType(),\n",
    "        \"AS69\": DoubleType(),\n",
    "        \"AS70\": DateType(),\n",
    "        \"AS71\": DateType(),\n",
    "        \"AS80\": DoubleType(),\n",
    "        \"AS81\": DoubleType(),\n",
    "        \"AS82\": DoubleType(),\n",
    "        \"AS83\": StringType(),\n",
    "        \"AS84\": StringType(),\n",
    "        \"AS85\": DoubleType(),\n",
    "        \"AS86\": DoubleType(),\n",
    "        \"AS87\": DateType(),\n",
    "        \"AS88\": DoubleType(),\n",
    "        \"AS89\": StringType(),\n",
    "        \"AS90\": DoubleType(),\n",
    "        \"AS91\": DateType(),\n",
    "        \"AS92\": StringType(),\n",
    "        \"AS93\": DoubleType(),\n",
    "        \"AS94\": StringType(),\n",
    "        \"AS100\": DoubleType(),\n",
    "        \"AS101\": DoubleType(),\n",
    "        \"AS102\": DoubleType(),\n",
    "        \"AS103\": DoubleType(),\n",
    "        \"AS104\": DoubleType(),\n",
    "        \"AS105\": DoubleType(),\n",
    "        \"AS106\": DoubleType(),\n",
    "        \"AS107\": DoubleType(),\n",
    "        \"AS108\": DoubleType(),\n",
    "        \"AS109\": DoubleType(),\n",
    "        \"AS110\": DoubleType(),\n",
    "        \"AS111\": StringType(),\n",
    "        \"AS112\": DateType(),\n",
    "        \"AS115\": DoubleType(),\n",
    "        \"AS116\": DoubleType(),\n",
    "        \"AS117\": DoubleType(),\n",
    "        \"AS118\": DoubleType(),\n",
    "        \"AS119\": DoubleType(),\n",
    "        \"AS120\": DoubleType(),\n",
    "        \"AS121\": BooleanType(),\n",
    "        \"AS122\": BooleanType(),\n",
    "        \"AS123\": StringType(),\n",
    "        \"AS124\": DateType(),\n",
    "        \"AS125\": DoubleType(),\n",
    "        \"AS126\": DoubleType(),\n",
    "        \"AS127\": DateType(),\n",
    "        \"AS128\": DoubleType(),\n",
    "        \"AS129\": StringType(),\n",
    "        \"AS130\": DateType(),\n",
    "        \"AS131\": BooleanType(),\n",
    "        \"AS132\": DoubleType(),\n",
    "        \"AS133\": DateType(),\n",
    "        \"AS134\": DateType(),\n",
    "        \"AS135\": DoubleType(),\n",
    "        \"AS136\": DoubleType(),\n",
    "        \"AS137\": DateType(),\n",
    "        \"AS138\": DoubleType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.col(col_name).contains(\"True\"))\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    col_name, F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def get_columns_collection(df):\n",
    "    \"\"\"\n",
    "    Get collection of dataframe columns divided by topic.\n",
    "\n",
    "    :param df: Asset bronze Spark dataframe.\n",
    "    :return cols_dict: collection of columns labelled by topic.\n",
    "    \"\"\"\n",
    "    cols_dict = {\n",
    "        \"general\": [\"ed_code\", \"year\", \"month\"]\n",
    "        + [f\"AS{i}\" for i in range(1, 15) if f\"AS{i}\" in df.columns],\n",
    "        \"obligor_info\": [f\"AS{i}\" for i in range(15, 50) if f\"AS{i}\" in df.columns],\n",
    "        \"loan_info\": [f\"AS{i}\" for i in range(50, 80) if f\"AS{i}\" in df.columns],\n",
    "        \"interest_rate\": [f\"AS{i}\" for i in range(80, 100) if f\"AS{i}\" in df.columns],\n",
    "        \"financial_info\": [f\"AS{i}\" for i in range(100, 115) if f\"AS{i}\" in df.columns],\n",
    "        \"performance_info\": [\n",
    "            f\"AS{i}\" for i in range(115, 146) if f\"AS{i}\" in df.columns\n",
    "        ]\n",
    "    }\n",
    "    return cols_dict\n",
    "\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_obligor_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract obligor info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"obligor_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .withColumn(\"AS19\", F.unix_timestamp(F.col(\"AS19\")))\n",
    "        .withColumn(\"AS20\", F.unix_timestamp(F.col(\"AS20\")))\n",
    "        .withColumn(\"AS31\", F.unix_timestamp(F.col(\"AS31\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_loan_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract loan info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"loan_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .withColumn(\"AS50\", F.unix_timestamp(F.col(\"AS50\")))\n",
    "        .withColumn(\"AS51\", F.unix_timestamp(F.col(\"AS51\")))\n",
    "        .withColumn(\"AS67\", F.unix_timestamp(F.col(\"AS67\")))\n",
    "        .withColumn(\"AS70\", F.unix_timestamp(F.col(\"AS70\")))\n",
    "        .withColumn(\"AS71\", F.unix_timestamp(F.col(\"AS71\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_interest_rate(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract interest rate dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"interest_rate\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .withColumn(\"AS87\", F.unix_timestamp(F.col(\"AS87\")))\n",
    "        .withColumn(\"AS91\", F.unix_timestamp(F.col(\"AS91\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_financial_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract financial info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"financial_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .withColumn(\"AS112\", F.unix_timestamp(F.col(\"AS112\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_performance_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract performance info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"performance_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .withColumn(\"AS124\", F.unix_timestamp(F.col(\"AS124\")))\n",
    "        .withColumn(\"AS127\", F.unix_timestamp(F.col(\"AS127\")))\n",
    "        .withColumn(\"AS130\", F.unix_timestamp(F.col(\"AS130\")))\n",
    "        .withColumn(\"AS133\", F.unix_timestamp(F.col(\"AS133\")))\n",
    "        .withColumn(\"AS134\", F.unix_timestamp(F.col(\"AS134\")))\n",
    "        .withColumn(\"AS137\", F.unix_timestamp(F.col(\"AS137\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/assets.parquet'\n",
    ").filter(\"iscurrent == 1\").drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\")\n",
    "assets_columns = get_columns_collection(bronze_df)\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(bronze_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "cleaned_df = cast_to_datatype(tmp_df2, run_props[\"ASSET_COLUMNS\"])\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(cleaned_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate obligor info dataframe\")\n",
    "obligor_info_df = process_obligor_info(cleaned_df, assets_columns)\n",
    "print(\"Generate loan info dataframe\")\n",
    "loan_info_df = process_loan_info(cleaned_df, assets_columns)\n",
    "print(\"Generate interest rate dataframe\")\n",
    "interest_rate_df = process_interest_rate(cleaned_df, assets_columns)\n",
    "print(\"Generate financial info dataframe\")\n",
    "financial_info_df = process_financial_info(cleaned_df, assets_columns)\n",
    "print(\"Generate performace info dataframe\")\n",
    "performance_info_df = process_performance_info(cleaned_df, assets_columns)\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    date_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/date_table.parquet\")\n",
    ")\n",
    "(\n",
    "    loan_info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/loan_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    obligor_info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/obligor_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    financial_info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/financial_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    interest_rate_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/interest_rate_table.parquet\")\n",
    ")\n",
    "(\n",
    "    performance_info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/assets/performance_info_table.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLATERAL SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\"CS11\", \"CS12\", \"CS22\"]\n",
    "    config[\"COLLATERAL_COLUMNS\"] = {\n",
    "        \"CS1\": StringType(),\n",
    "        \"CS2\": StringType(),\n",
    "        \"CS3\": StringType(),\n",
    "        \"CS4\": DoubleType(),\n",
    "        \"CS5\": DoubleType(),\n",
    "        \"CS6\": StringType(),\n",
    "        \"CS7\": BooleanType(),\n",
    "        \"CS8\": BooleanType(),\n",
    "        \"CS9\": BooleanType(),\n",
    "        \"CS10\": DoubleType(),\n",
    "        \"CS11\": DateType(),\n",
    "        \"CS12\": DateType(),\n",
    "        \"CS13\": StringType(),\n",
    "        \"CS14\": StringType(),\n",
    "        \"CS15\": DoubleType(),\n",
    "        \"CS16\": StringType(),\n",
    "        \"CS17\": StringType(),\n",
    "        \"CS18\": DoubleType(),\n",
    "        \"CS19\": DoubleType(),\n",
    "        \"CS20\": StringType(),\n",
    "        \"CS21\": DoubleType(),\n",
    "        \"CS22\": DateType(),\n",
    "        \"CS23\": StringType(),\n",
    "        \"CS24\": StringType(),\n",
    "        \"CS25\": StringType(),\n",
    "        \"CS26\": StringType(),\n",
    "        \"CS27\": StringType(),\n",
    "        \"CS28\": DoubleType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.col(col_name).contains(\"True\"))\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    col_name, F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_collateral_info(df):\n",
    "    \"\"\"\n",
    "    Extract collateral info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\n",
    "            \"CS11\", F.unix_timestamp(F.to_timestamp(F.col(\"CS11\"), \"yyyy-MM\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"CS12\", F.unix_timestamp(F.to_timestamp(F.col(\"CS12\"), \"yyyy-MM\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"CS22\", F.unix_timestamp(F.to_timestamp(F.col(\"CS22\"), \"yyyy-MM\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start COLLATERAL SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/collaterals.parquet'\n",
    ").filter(\"iscurrent == 1\").drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\")\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(bronze_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "cleaned_df = cast_to_datatype(tmp_df2, run_props[\"COLLATERAL_COLUMNS\"])\n",
    "print(\"Generate collateral info dataframe\")\n",
    "info_df = process_collateral_info(cleaned_df)\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(cleaned_df, run_props[\"DATE_COLUMNS\"])\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/collaterals/info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    date_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/collaterals/date_table.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND INFO SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\"BS1\", \"BS27\", \"BS28\", \"BS38\", \"BS39\"]\n",
    "    config[\"BOND_COLUMNS\"] = {\n",
    "        \"BS1\": DateType(),\n",
    "        \"BS2\": StringType(),\n",
    "        \"BS3\": DoubleType(),\n",
    "        \"BS4\": DoubleType(),\n",
    "        \"BS5\": BooleanType(),\n",
    "        \"BS6\": StringType(),\n",
    "        \"BS11\": DoubleType(),\n",
    "        \"BS12\": BooleanType(),\n",
    "        \"BS13\": DoubleType(),\n",
    "        \"BS19\": StringType(),\n",
    "        \"BS20\": StringType(),\n",
    "        \"BS25\": StringType(),\n",
    "        \"BS26\": StringType(),\n",
    "        \"BS27\": DateType(),\n",
    "        \"BS28\": DateType(),\n",
    "        \"BS29\": StringType(),\n",
    "        \"BS30\": DoubleType(),\n",
    "        \"BS31\": DoubleType(),\n",
    "        \"BS32\": StringType(),\n",
    "        \"BS33\": DoubleType(),\n",
    "        \"BS34\": DoubleType(),\n",
    "        \"BS35\": DoubleType(),\n",
    "        \"BS36\": DoubleType(),\n",
    "        \"BS37\": DoubleType(),\n",
    "        \"BS38\": DateType(),\n",
    "        \"BS39\": DateType()\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.col(col_name).contains(\"True\"))\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    col_name, F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def get_columns_collection(df):\n",
    "    \"\"\"\n",
    "    Get collection of dataframe columns divided by topic.\n",
    "\n",
    "    :param df: Asset bronze Spark dataframe.\n",
    "    :return cols_dict: collection of columns labelled by topic.\n",
    "    \"\"\"\n",
    "    cols_dict = {\n",
    "        \"bond_info\":[\"ed_code\", \"year\", \"month\"]\n",
    "        + [f\"BS{i}\" for i in range(1, 11) if f\"BS{i}\" in df.columns],\n",
    "        \"collateral_info\": [\"ed_code\", \"year\", \"month\"]\n",
    "        + [f\"BS{i}\" for i in range(11, 19) if f\"BS{i}\" in df.columns],\n",
    "        \"contact_info\": [\"ed_code\", \"year\", \"month\"]\n",
    "        + [f\"BS{i}\" for i in range(19, 25) if f\"BS{i}\" in df.columns],\n",
    "        \"tranche_info\": [\"ed_code\", \"year\", \"month\"]\n",
    "        + [f\"BS{i}\" for i in range(25, 40) if f\"BS{i}\" in df.columns],\n",
    "    }\n",
    "    return cols_dict\n",
    "\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_bond_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract bond info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"bond_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\n",
    "            \"BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_collateral_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract collateral info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select([\"BS1\", \"BS2\"] + cols_dict[\"collateral_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\n",
    "            \"BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_contact_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract contact info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select([\"BS1\", \"BS2\"] + cols_dict[\"contact_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\n",
    "            \"BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "def process_tranche_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract tranche info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select([\"BS1\", \"BS2\"] + cols_dict[\"tranche_info\"])\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\n",
    "            \"BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"BS27\", F.unix_timestamp(F.to_timestamp(F.col(\"BS27\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"BS28\", F.unix_timestamp(F.to_timestamp(F.col(\"BS28\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"BS38\", F.unix_timestamp(F.to_timestamp(F.col(\"BS38\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"BS39\", F.unix_timestamp(F.to_timestamp(F.col(\"BS39\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start BOND INFO SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/bond_info.parquet'\n",
    ").filter(\"iscurrent == 1\").drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\")\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(bronze_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "cleaned_df = cast_to_datatype(tmp_df2, run_props[\"BOND_COLUMNS\"])\n",
    "bond_info_columns = get_columns_collection(cleaned_df)\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(cleaned_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate bond info dataframe\")\n",
    "info_df = process_bond_info(cleaned_df, bond_info_columns)\n",
    "print(\"Generate collateral info dataframe\")\n",
    "collateral_df = process_collateral_info(cleaned_df, bond_info_columns)\n",
    "print(\"Generate contact info dataframe\")\n",
    "contact_df = process_contact_info(cleaned_df, bond_info_columns)\n",
    "print(\"Generate tranche info dataframe\")\n",
    "tranche_df = process_tranche_info(cleaned_df, bond_info_columns)\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    date_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/bond_info/date_table.parquet\")\n",
    ")\n",
    "(\n",
    "    info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/bond_info/info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    collateral_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/bond_info/collaterals_table.parquet\")\n",
    ")\n",
    "(\n",
    "    contact_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/bond_info/contacts_table.parquet\")\n",
    ")\n",
    "(\n",
    "    tranche_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/output/silver/bond_info/trache_info_table.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMORTISATION SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start AMORTISATION SILVER job.\n",
      "Cast data to correct types.\n",
      "Generate time dataframe\n",
      "Generate info dataframe\n"
     ]
    }
   ],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"AMORTISATION_COLUMNS\"] = {\n",
    "        \"AS3\": StringType(),\n",
    "        \"AS150\": DoubleType(),\n",
    "        \"AS151\": DateType(),\n",
    "        \"AS1348\": DoubleType(),\n",
    "        \"AS1349\": DateType(),\n",
    "    }\n",
    "    for i in range(152, 1348):\n",
    "        if i % 2 == 0:\n",
    "            config[\"AMORTISATION_COLUMNS\"][f\"AS{i}\"] = DoubleType()\n",
    "        else:\n",
    "            config[\"AMORTISATION_COLUMNS\"][f\"AS{i}\"] = DateType()\n",
    "    return config\n",
    "\n",
    "\n",
    "def _melt(df, id_vars, value_vars, var_name=\"FEATURE_NAME\", value_name=\"FEATURE_VALUE\"):\n",
    "    \"\"\"Convert DataFrame from wide to long format.\"\"\"\n",
    "    # Ref:https://stackoverflow.com/a/41673644\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(\n",
    "        *(\n",
    "            F.struct(F.lit(c).alias(var_name), F.col(c).alias(value_name))\n",
    "            for c in value_vars\n",
    "        )\n",
    "    )\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "    cols = id_vars + [\n",
    "        F.col(\"_vars_and_vals\")[x].cast(\"string\").alias(x)\n",
    "        for x in [var_name, value_name]\n",
    "    ]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n",
    "def unpivot_dataframe(df, columns):\n",
    "    \"\"\"\n",
    "    Convert dataframe from wide to long table.\n",
    "\n",
    "    :param df: raw Spark dataframe.\n",
    "    :param columns: data columns with respective datatype.\n",
    "    :return new_df: unpivot Spark dataframe.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"AS3\", F.concat_ws(\"_\",F.col(\"AS3\"), F.monotonically_increasing_id()))\n",
    "    date_columns = [\n",
    "        k for k, v in columns.items() if v == DateType() and k in df.columns\n",
    "    ]\n",
    "    double_columns = [\n",
    "        k for k, v in columns.items() if v == DoubleType() and k in df.columns\n",
    "    ]\n",
    "\n",
    "    date_df = (\n",
    "            _melt(\n",
    "                df,\n",
    "                id_vars=[\"AS3\"],\n",
    "                value_vars=date_columns,\n",
    "                var_name=\"DATE_COLUMNS\",\n",
    "                value_name=\"DATE_VALUE\",\n",
    "                )\n",
    "            .filter(F.col(\"DATE_VALUE\").isNotNull())\n",
    "        )\n",
    "    double_df = (\n",
    "            _melt(\n",
    "                df,\n",
    "                id_vars=[\"AS3\"],\n",
    "                value_vars=double_columns,\n",
    "                var_name=\"DOUBLE_COLUMNS\",\n",
    "                value_name=\"DOUBLE_VALUE\",\n",
    "                )\n",
    "        )\n",
    "    scd2_df = df.select(\"AS3\",\"ed_code\", \"year\", \"month\")\n",
    "    new_df = (  \n",
    "        date_df\n",
    "        .join(double_df, on=\"AS3\", how=\"inner\")\n",
    "        .join(scd2_df, on=\"AS3\", how=\"inner\")\n",
    "        .withColumn(\"AS3\", F.split(F.col(\"AS3\"),\"_\").getItem(0))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == DateType():\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "        if data_type == DoubleType():\n",
    "            df = df.withColumn(col_name, F.round(F.col(col_name).cast(DoubleType()), 2))\n",
    "    return df\n",
    "\n",
    "def process_dates(df):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(\"DATE_VALUE\")\n",
    "        .withColumnRenamed(\"DATE_VALUE\", \"date_col\")\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "def process_info(df):\n",
    "    \"\"\"\n",
    "    Extract amortisation values dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = df.withColumn(\"DATE_VALUE\", F.unix_timestamp(F.col(\"DATE_VALUE\")))\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start AMORTISATION SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/amortisation.parquet'\n",
    ").filter(\"iscurrent == 1\").drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\")\n",
    "print(\"Cast data to correct types.\")\n",
    "tmp_df1 = unpivot_dataframe(bronze_df, run_props[\"AMORTISATION_COLUMNS\"])\n",
    "cleaned_df = (\n",
    "    tmp_df1.withColumn(\"DATE_VALUE\", F.to_date(F.col(\"DATE_VALUE\")))\n",
    "    .withColumn(\n",
    "        \"DOUBLE_VALUE\", F.round(F.col(\"DOUBLE_VALUE\").cast(DoubleType()), 2)\n",
    "    )\n",
    ")\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(cleaned_df)\n",
    "print(\"Generate info dataframe\")\n",
    "info_df = process_info(cleaned_df)\n",
    "# print(\"Write dataframe\")\n",
    "\n",
    "# (\n",
    "#     date_df.write\n",
    "#     .mode(\"overwrite\")\n",
    "#     .parquet(\"../data/output/silver/amortisation/date_table.parquet\")\n",
    "# )\n",
    "# (\n",
    "#     info_df.write\n",
    "#     .partitionBy(\"year\", \"month\")\n",
    "#     .mode(\"overwrite\")\n",
    "#     .parquet(\"../data/output/silver/amortisation/info_table.parquet\")\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEAL DETAILS SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start DEAL DETAILS SILVER job.\n",
      "Cast data to correct types.\n",
      "Generate time dataframe\n",
      "Generate deal info dataframe\n"
     ]
    }
   ],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\n",
    "        \"PoolCreationDate\",\n",
    "        \"RestructureDates\",\n",
    "        \"InterestPaymentDate\",\n",
    "        \"PoolCutOffDate\",\n",
    "        \"SubmissionTimestamp\",\n",
    "    ]\n",
    "    config[\"DEAL_DETAILS_COLUMNS\"] = {\n",
    "        \"AssetClassCode\": StringType(),\n",
    "        \"AssetClassName\": StringType(),\n",
    "        \"CountryCodeOfPrimaryExchange\": StringType(),\n",
    "        \"CountryCodeOfSecuritisedAsset\": StringType(),\n",
    "        \"CountryCodeOfSpvIncorporation\": StringType(),\n",
    "        \"CountryOfPrimaryExchange\": StringType(),\n",
    "        \"CountryOfSecuritisedAsset\": StringType(),\n",
    "        \"CountryOfSpvIncorporation\": StringType(),\n",
    "        \"DataOwner\": StringType(),\n",
    "        \"DataProvider\": StringType(),\n",
    "        \"DealSize\": DoubleType(),\n",
    "        \"DealVersion\": IntegerType(),\n",
    "        \"ed_code\": StringType(),\n",
    "        \"ISIN\": StringType(),\n",
    "        \"IsActiveDeal\": BooleanType(),\n",
    "        \"IsECBEligible\": BooleanType(),\n",
    "        \"IsMasterTrust\": BooleanType(),\n",
    "        \"IsProvisional\": BooleanType(),\n",
    "        \"IsRestructured\": BooleanType(),\n",
    "        \"PoolCreationDate\": DateType(),\n",
    "        \"RestructureDates\": DateType(),\n",
    "        \"SpvName\": StringType(),\n",
    "        \"ContactInformation\": StringType(),\n",
    "        \"CurrentLLPDUploadStatus\": StringType(),\n",
    "        \"CurrentPoolBalance\": DoubleType(),\n",
    "        \"ECBDataQualityScore\": StringType(),\n",
    "        \"HasSuccessfulSubmission\": BooleanType(),\n",
    "        \"InterestPaymentDate\": DateType(),\n",
    "        \"NumberOfActiveAssets\": IntegerType(),\n",
    "        \"OriginalPoolBalance\": DoubleType(),\n",
    "        \"PoolCutOffDate\": DateType(),\n",
    "        \"RequestId\": StringType(),\n",
    "        \"SubmissionTimestamp\": DateType(),\n",
    "        \"TotalNumberOfAssets\": IntegerType(),\n",
    "        \"TotalResubmissionCount\": IntegerType(),\n",
    "        \"TotalNotionalValue\": DoubleType(),\n",
    "        \"Vintage\": IntegerType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan deal details data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = df.withColumn(col_name, F.col(col_name).contains(\"true\"))\n",
    "        if data_type == DateType():\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "        if data_type == DoubleType():\n",
    "            df = df.withColumn(col_name, F.round(F.col(col_name).cast(DoubleType()), 2))\n",
    "        if data_type == IntegerType():\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_deal_info(df):\n",
    "    \"\"\"\n",
    "    Extract deal info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.dropDuplicates()\n",
    "        .withColumn(\"PoolCreationDate\", F.unix_timestamp(F.col(\"PoolCreationDate\")))\n",
    "        .withColumn(\"RestructureDates\", F.unix_timestamp(F.col(\"RestructureDates\")))\n",
    "        .withColumn(\n",
    "            \"InterestPaymentDate\", F.unix_timestamp(F.col(\"InterestPaymentDate\"))\n",
    "        )\n",
    "        .withColumn(\"PoolCutOffDate\", F.unix_timestamp(F.col(\"PoolCutOffDate\")))\n",
    "        .withColumn(\n",
    "            \"SubmissionTimestamp\", F.unix_timestamp(F.col(\"SubmissionTimestamp\"))\n",
    "        )\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# def return_write_mode(bucket_name, prefix, pcds):\n",
    "#     \"\"\"\n",
    "#     If PCDs are already presents as partition return \"overwrite\", otherwise \"append\" mode.\n",
    "\n",
    "#     :param bucket_name: GS bucket where files are stored.\n",
    "#     :param prefix: specific bucket prefix from where to collect files.\n",
    "#     :param pcds: list of PCDs that have been elaborated in the previous Silver layer.\n",
    "#     :return write_mode: label that express how data should be written on storage.\n",
    "#     \"\"\"\n",
    "#     storage_client = storage.Client(project=\"dataops-369610\")\n",
    "#     check_list = []\n",
    "#     for pcd in pcds:\n",
    "#         year = pcd.split(\"-\")[0]\n",
    "#         month = pcd.split(\"-\")[1]\n",
    "#         partition_prefix = f\"{prefix}/year={year}/month={month}\"\n",
    "#         check_list.append(\n",
    "#             len(\n",
    "#                 [\n",
    "#                     b.name\n",
    "#                     for b in storage_client.list_blobs(\n",
    "#                         bucket_name, prefix=partition_prefix\n",
    "#                     )\n",
    "#                 ]\n",
    "#             )\n",
    "#         )\n",
    "#     if sum(check_list) > 0:\n",
    "#         return \"overwrite\"\n",
    "#     else:\n",
    "#         return \"append\"\n",
    "\n",
    "\n",
    "print(\"Start DEAL DETAILS SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/deal_details.parquet'\n",
    ").filter(\"iscurrent == 1\").drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\")\n",
    "print(\"Cast data to correct types.\")\n",
    "cleaned_df = cast_to_datatype(bronze_df, run_props[\"DEAL_DETAILS_COLUMNS\"])\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(cleaned_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate deal info dataframe\")\n",
    "deal_info_df = process_deal_info(cleaned_df)\n",
    "\n",
    "# print(\"Write dataframe\")\n",
    "# write_mode = return_write_mode(bucket_name, silver_prefix, pcds)\n",
    "\n",
    "# (\n",
    "#     date_df.write.format(\"delta\")\n",
    "#     .partitionBy(\"year\", \"month\")\n",
    "#     .mode(write_mode)\n",
    "#     .save(f\"gs://{bucket_name}/{silver_prefix}/date_table\")\n",
    "# )\n",
    "# (\n",
    "#     deal_info_df.write.format(\"delta\")\n",
    "#     .partitionBy(\"year\", \"month\")\n",
    "#     .mode(write_mode)\n",
    "#     .save(f\"gs://{bucket_name}/{silver_prefix}/deal_info_table\")\n",
    "# )\n",
    "# print(\"End DEAL DETAILS SILVER job.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            date_col|\n",
      "+--------------------+\n",
      "| 2017-04-26T00:00:00|\n",
      "| 2017-03-31T00:00:00|\n",
      "|2017-05-08T09:58:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    bronze_df\n",
    "    .select(F.explode(F.array(run_props[\"DATE_COLUMNS\"])).alias(\"date_col\"))\n",
    "    .filter(\"date_col IS NOT NULL\")\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------------------------+-----------------------------+-----------------------------+------------------------+-------------------------+-------------------------+--------------------+--------------------+--------+-----------+--------------------+--------------------+------------+-------------+-------------+-------------+--------------+----------------+----------------+--------------------+--------------------+-----------------------+------------------+-------------------+-----------------------+-------------------+--------------------+-------------------+--------------+--------------------+-------------------+-------------------+----------------------+------------------+-------+----+-----+\n",
      "|AssetClassCode|      AssetClassName|CountryCodeOfPrimaryExchange|CountryCodeOfSecuritisedAsset|CountryCodeOfSpvIncorporation|CountryOfPrimaryExchange|CountryOfSecuritisedAsset|CountryOfSpvIncorporation|           DataOwner|        DataProvider|DealSize|DealVersion|             ed_code|                ISIN|IsActiveDeal|IsECBEligible|IsMasterTrust|IsProvisional|IsRestructured|PoolCreationDate|RestructureDates|             SpvName|  ContactInformation|CurrentLLPDUploadStatus|CurrentPoolBalance|ECBDataQualityScore|HasSuccessfulSubmission|InterestPaymentDate|NumberOfActiveAssets|OriginalPoolBalance|PoolCutOffDate|           RequestId|SubmissionTimestamp|TotalNumberOfAssets|TotalResubmissionCount|TotalNotionalValue|Vintage|year|month|\n",
      "+--------------+--------------------+----------------------------+-----------------------------+-----------------------------+------------------------+-------------------------+-------------------------+--------------------+--------------------+--------+-----------+--------------------+--------------------+------------+-------------+-------------+-------------+--------------+----------------+----------------+--------------------+--------------------+-----------------------+------------------+-------------------+-----------------------+-------------------+--------------------+-------------------+--------------+--------------------+-------------------+-------------------+----------------------+------------------+-------+----+-----+\n",
      "|           SME|Small and Medium-...|                          ES|                           ES|                           ES|                   Spain|                    Spain|                    Spain|BANCO SABADELL, S.A.|InterMoney Tituli...|   750.0|          3|SMESES00017610032...|ES0339758007;ES03...|       false|        false|        false|        false|         false|            null|            null|FTPYME TDA CAM 2,...|Isabel Garrido, i...|                Success|              21.8|                 A1|                   true|         1493164800|                 408|              106.7|    1490918400|33e7048c-6ba7-475...|         1494201600|                408|                     0|              null|   2004|2017|    3|\n",
      "+--------------+--------------------+----------------------------+-----------------------------+-----------------------------+------------------------+-------------------------+-------------------------+--------------------+--------------------+--------+-----------+--------------------+--------------------+------------+-------------+-------------+-------------+--------------+----------------+----------------+--------------------+--------------------+-----------------------+------------------+-------------------+-----------------------+-------------------+--------------------+-------------------+--------------+--------------------+-------------------+-------------------+----------------------+------------------+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deal_info_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
