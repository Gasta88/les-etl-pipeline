{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main ETL for Loan Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, StringType, DoubleType, BooleanType\n",
    "import csv\n",
    "from functools import reduce\n",
    "import os\n",
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "\n",
    "SPARK = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "BRONZE_SOURCE_DIR = \"../data/SMES_IT_ES_FR\"\n",
    "SILVER_SOURCE_DIR = \"../data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSET BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Loan_Data\"\n",
    "    config[\"ASSET_COLUMNS\"] = {\n",
    "        \"AS1\": DateType(),\n",
    "        \"AS2\": StringType(),\n",
    "        \"AS3\": StringType(),\n",
    "        \"AS4\": StringType(),\n",
    "        \"AS5\": StringType(),\n",
    "        \"AS6\": StringType(),\n",
    "        \"AS7\": StringType(),\n",
    "        \"AS8\": StringType(),\n",
    "        \"AS15\": StringType(),\n",
    "        \"AS16\": StringType(),\n",
    "        \"AS17\": StringType(),\n",
    "        \"AS18\": StringType(),\n",
    "        \"AS19\": DateType(),\n",
    "        \"AS20\": DateType(),\n",
    "        \"AS21\": StringType(),\n",
    "        \"AS22\": StringType(),\n",
    "        \"AS23\": BooleanType(),\n",
    "        \"AS24\": StringType(),\n",
    "        \"AS25\": StringType(),\n",
    "        \"AS26\": StringType(),\n",
    "        \"AS27\": DoubleType(),\n",
    "        \"AS28\": DoubleType(),\n",
    "        \"AS29\": BooleanType(),\n",
    "        \"AS30\": DoubleType(),\n",
    "        \"AS31\": DateType(),\n",
    "        \"AS32\": StringType(),\n",
    "        \"AS33\": StringType(),\n",
    "        \"AS34\": StringType(),\n",
    "        \"AS35\": StringType(),\n",
    "        \"AS36\": StringType(),\n",
    "        \"AS37\": DoubleType(),\n",
    "        \"AS38\": DoubleType(),\n",
    "        \"AS39\": DoubleType(),\n",
    "        \"AS40\": DoubleType(),\n",
    "        \"AS41\": DoubleType(),\n",
    "        \"AS42\": StringType(),\n",
    "        \"AS43\": StringType(),\n",
    "        \"AS44\": DoubleType(),\n",
    "        \"AS45\": StringType(),\n",
    "        \"AS50\": DateType(),\n",
    "        \"AS51\": DateType(),\n",
    "        \"AS52\": StringType(),\n",
    "        \"AS53\": BooleanType(),\n",
    "        \"AS54\": DoubleType(),\n",
    "        \"AS55\": DoubleType(),\n",
    "        \"AS56\": DoubleType(),\n",
    "        \"AS57\": StringType(),\n",
    "        \"AS58\": StringType(),\n",
    "        \"AS59\": StringType(),\n",
    "        \"AS60\": DoubleType(),\n",
    "        \"AS61\": DoubleType(),\n",
    "        \"AS62\": StringType(),\n",
    "        \"AS63\": DoubleType(),\n",
    "        \"AS64\": DoubleType(),\n",
    "        \"AS65\": StringType(),\n",
    "        \"AS66\": DoubleType(),\n",
    "        \"AS67\": DateType(),\n",
    "        \"AS68\": StringType(),\n",
    "        \"AS69\": DoubleType(),\n",
    "        \"AS70\": DateType(),\n",
    "        \"AS71\": DateType(),\n",
    "        \"AS80\": DoubleType(),\n",
    "        \"AS81\": DoubleType(),\n",
    "        \"AS82\": DoubleType(),\n",
    "        \"AS83\": StringType(),\n",
    "        \"AS84\": StringType(),\n",
    "        \"AS85\": DoubleType(),\n",
    "        \"AS86\": DoubleType(),\n",
    "        \"AS87\": DateType(),\n",
    "        \"AS88\": DoubleType(),\n",
    "        \"AS89\": StringType(),\n",
    "        \"AS90\": DoubleType(),\n",
    "        \"AS91\": DateType(),\n",
    "        \"AS92\": StringType(),\n",
    "        \"AS93\": DoubleType(),\n",
    "        \"AS94\": StringType(),\n",
    "        \"AS100\": DoubleType(),\n",
    "        \"AS101\": DoubleType(),\n",
    "        \"AS102\": DoubleType(),\n",
    "        \"AS103\": DoubleType(),\n",
    "        \"AS104\": DoubleType(),\n",
    "        \"AS105\": DoubleType(),\n",
    "        \"AS106\": DoubleType(),\n",
    "        \"AS107\": DoubleType(),\n",
    "        \"AS108\": DoubleType(),\n",
    "        \"AS109\": DoubleType(),\n",
    "        \"AS110\": DoubleType(),\n",
    "        \"AS111\": StringType(),\n",
    "        \"AS112\": DateType(),\n",
    "        \"AS115\": DoubleType(),\n",
    "        \"AS116\": DoubleType(),\n",
    "        \"AS117\": DoubleType(),\n",
    "        \"AS118\": DoubleType(),\n",
    "        \"AS119\": DoubleType(),\n",
    "        \"AS120\": DoubleType(),\n",
    "        \"AS121\": BooleanType(),\n",
    "        \"AS122\": BooleanType(),\n",
    "        \"AS123\": StringType(),\n",
    "        \"AS124\": DateType(),\n",
    "        \"AS125\": DoubleType(),\n",
    "        \"AS126\": DoubleType(),\n",
    "        \"AS127\": DateType(),\n",
    "        \"AS128\": DoubleType(),\n",
    "        \"AS129\": StringType(),\n",
    "        \"AS130\": DateType(),\n",
    "        \"AS131\": BooleanType(),\n",
    "        \"AS132\": DoubleType(),\n",
    "        \"AS133\": DateType(),\n",
    "        \"AS134\": DateType(),\n",
    "        \"AS135\": DoubleType(),\n",
    "        \"AS136\": DoubleType(),\n",
    "        \"AS137\": DateType(),\n",
    "        \"AS138\": DoubleType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: list of desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [\n",
    "        f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\") if \"Labeled0M\" not in f\n",
    "    ]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            portfolio_id = csv_f.split(\"/\")[-2]\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = spark.createDataFrame(content, col_names).withColumn(\n",
    "                \"ID\", F.lit(portfolio_id)\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.col(col_name).contains(\"True\"))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.to_date(F.col(col_name)))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    \"tmp_col_name\", F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "    df = (\n",
    "        df.withColumn(\"year\", F.year(F.col(\"AS1\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"AS1\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"AS1\")))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Start ASSETS BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_asset_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_asset_files)} asset data files.\")\n",
    "raw_asset_df = create_dataframe(SPARK, all_asset_files)\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(raw_asset_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "final_df = cast_to_datatype(tmp_df2, run_props[\"ASSET_COLUMNS\"])\n",
    "(\n",
    "    final_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../dataoutput/bronze/assets.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLATERAL BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Collateral\"\n",
    "    config[\"COLLATERAL_COLUMNS\"] = {\n",
    "        \"CS1\": StringType(),\n",
    "        \"CS2\": StringType(),\n",
    "        \"CS3\": StringType(),\n",
    "        \"CS4\": DoubleType(),\n",
    "        \"CS5\": DoubleType(),\n",
    "        \"CS6\": StringType(),\n",
    "        \"CS7\": BooleanType(),\n",
    "        \"CS8\": BooleanType(),\n",
    "        \"CS9\": BooleanType(),\n",
    "        \"CS10\": DoubleType(),\n",
    "        \"CS11\": DateType(),\n",
    "        \"CS12\": DateType(),\n",
    "        \"CS13\": StringType(),\n",
    "        \"CS14\": StringType(),\n",
    "        \"CS15\": DoubleType(),\n",
    "        \"CS16\": StringType(),\n",
    "        \"CS17\": StringType(),\n",
    "        \"CS18\": DoubleType(),\n",
    "        \"CS19\": DoubleType(),\n",
    "        \"CS20\": StringType(),\n",
    "        \"CS21\": DoubleType(),\n",
    "        \"CS22\": DateType(),\n",
    "        \"CS23\": StringType(),\n",
    "        \"CS24\": StringType(),\n",
    "        \"CS25\": StringType(),\n",
    "        \"CS26\": StringType(),\n",
    "        \"CS27\": StringType(),\n",
    "        \"CS28\": DoubleType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            portfolio_id = csv_f.split(\"/\")[-2]\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = spark.createDataFrame(content, col_names).withColumn(\n",
    "                \"ID\", F.lit(portfolio_id)\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.col(col_name).contains(\"True\"))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.to_date(F.col(col_name)))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    \"tmp_col_name\", F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "    df = (\n",
    "        df.withColumn(\"year\", F.year(F.col(\"AS1\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"AS1\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Start COLLATERAL BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_collateral_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_collateral_files)} collateral data files.\")\n",
    "tmp_raw_collateral_df = create_dataframe(SPARK, all_collateral_files)\n",
    "try:\n",
    "    assets_bronze_df = (\n",
    "        SPARK\n",
    "        .read.parquet(f'{run_props[\"SOURCE_DIR\"]}/bronze/assets.parquet')\n",
    "        .select(\"AS1\", \"AS3\")\n",
    "        .withColumnRenamed(\"AS3\", \"CS2\")\n",
    "    )\n",
    "    raw_collateral_df = tmp_raw_collateral_df.join(\n",
    "        assets_bronze_df, on=\"CS2\", how=\"inner\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"No bronze asset dataframe found. Exit process!\")\n",
    "    sys.exit(1)\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(raw_collateral_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "final_df = cast_to_datatype(tmp_df2, run_props[\"COLLATERAL_COLUMNS\"])\n",
    "(\n",
    "    final_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../dataoutput/bronze/collaterals.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND INFO BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Bond_Info\"\n",
    "    config[\"BOND_COLUMNS\"] = {\n",
    "        \"BS1\": DateType(),\n",
    "        \"BS2\": StringType(),\n",
    "        \"BS3\": DoubleType(),\n",
    "        \"BS4\": DoubleType(),\n",
    "        \"BS5\": BooleanType(),\n",
    "        \"BS6\": StringType(),\n",
    "        \"BS11\": DoubleType(),\n",
    "        \"BS12\": BooleanType(),\n",
    "        \"BS13\": DoubleType(),\n",
    "        \"BS19\": StringType(),\n",
    "        \"BS20\": StringType(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            portfolio_id = csv_f.split(\"/\")[-2]\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = spark.createDataFrame(content, col_names).withColumn(\n",
    "                \"ID\", F.lit(portfolio_id)\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "def replace_no_data(df):\n",
    "    \"\"\"\n",
    "    Replace ND values inside the dataframe\n",
    "    TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "          Should handle this information better in future releases.\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without ND values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_bool_data(df):\n",
    "    \"\"\"\n",
    "    Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :return df: Spark dataframe without Y/N values.\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "            .when(F.col(col_name) == \"N\", \"False\")\n",
    "            .otherwise(F.col(col_name)),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_to_datatype(df, columns):\n",
    "    \"\"\"\n",
    "    Cast data to the respective datatype.\n",
    "\n",
    "    :param df: Spark dataframe with loan asset data.\n",
    "    :param columns: collection of column names and respective data types.\n",
    "    :return df: Spark dataframe with correct values.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in columns.items():\n",
    "        if data_type == BooleanType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.col(col_name).contains(\"True\"))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DateType():\n",
    "            df = (\n",
    "                df.withColumn(\"tmp_col_name\", F.to_date(F.col(col_name)))\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "        if data_type == DoubleType():\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    \"tmp_col_name\", F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "                )\n",
    "                .drop(col_name)\n",
    "                .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "            )\n",
    "    df = (\n",
    "        df.withColumn(\"year\", F.year(F.col(\"BS1\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"BS1\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"BS1\")))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Start BOND INFO BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_bond_info_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_bond_info_files)} bond info data files.\")\n",
    "raw_bond_info_df = create_dataframe(SPARK, all_bond_info_files)\n",
    "print(\"Remove ND values.\")\n",
    "tmp_df1 = replace_no_data(raw_bond_info_df)\n",
    "print(\"Replace Y/N with boolean flags.\")\n",
    "tmp_df2 = replace_bool_data(tmp_df1)\n",
    "print(\"Cast data to correct types.\")\n",
    "final_df = cast_to_datatype(tmp_df2, run_props[\"BOND_COLUMNS\"])\n",
    "(\n",
    "    final_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../dataoutput/bronze/bond_info.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMORTISATION BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Amortization\"\n",
    "    config[\"AMORTISATION_COLUMNS\"] = {\n",
    "        \"AS3\": StringType(),\n",
    "        \"AS150\": DoubleType(),\n",
    "        \"AS151\": DateType(),\n",
    "        \"AS1348\": DoubleType(),\n",
    "        \"AS1349\": DateType(),\n",
    "    }\n",
    "    for i in range(152, 1348):\n",
    "        if i % 2 == 0:\n",
    "            config[\"BOND_COLUMNS\"][f\"AS{i}\"] = DoubleType()\n",
    "        else:\n",
    "            config[\"BOND_COLUMNS\"][f\"AS{i}\"] = DateType()\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            portfolio_id = csv_f.split(\"/\")[-2]\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(\n",
    "                        list(filter(None, [None if x == \"\" else x for x in line]))\n",
    "                    )\n",
    "            df = spark.createDataFrame(content, col_names).withColumn(\n",
    "                \"ID\", F.lit(portfolio_id)\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "def _melt(df, id_vars, value_vars, var_name=\"FEATURE_NAME\", value_name=\"FEATURE_VALUE\"):\n",
    "    \"\"\"Convert DataFrame from wide to long format.\"\"\"\n",
    "    # Ref:https://stackoverflow.com/a/41673644\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(\n",
    "        *(\n",
    "            F.struct(F.lit(c).alias(var_name), F.col(c).alias(value_name))\n",
    "            for c in value_vars\n",
    "        )\n",
    "    )\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "    cols = id_vars + [\n",
    "        F.col(\"_vars_and_vals\")[x].cast(\"string\").alias(x)\n",
    "        for x in [var_name, value_name]\n",
    "    ]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n",
    "def unpivot_dataframe(df, columns):\n",
    "    \"\"\"\n",
    "    Convert dataframe from wide to long table.\n",
    "\n",
    "    :param df: raw Spark dataframe.\n",
    "    :param columns: data columns with respective datatype.\n",
    "    :return new_df: unpivot Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_columns = [\n",
    "        k for k, v in columns.items() if v == DateType() and k in df.columns\n",
    "    ]\n",
    "    double_columns = [\n",
    "        k for k, v in columns.items() if v == DoubleType() and k in df.columns\n",
    "    ]\n",
    "\n",
    "    date_df = _melt(\n",
    "        df,\n",
    "        id_vars=[\"AS3\"],\n",
    "        value_vars=date_columns,\n",
    "        var_name=\"DATE_COLUMNS\",\n",
    "        value_name=\"DATE_VALUE\",\n",
    "    )\n",
    "    double_df = _melt(\n",
    "        df,\n",
    "        id_vars=[\"AS3\"],\n",
    "        value_vars=double_columns,\n",
    "        var_name=\"DOUBLE_COLUMNS\",\n",
    "        value_name=\"DOUBLE_VALUE\",\n",
    "    )\n",
    "    new_df = date_df.join(double_df, on=\"AS3\", how=\"inner\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start AMORTISATION BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_amortisation_files = get_raw_files(\n",
    "    run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"]\n",
    ")\n",
    "print(f\"Retrieved {len(all_amortisation_files)} amortisation data files.\")\n",
    "raw_amortisation_df = create_dataframe(SPARK, all_amortisation_files)\n",
    "print(\"Unpivot amortisation dataframe.\")\n",
    "unpivot_df = unpivot_dataframe(\n",
    "    raw_amortisation_df, run_props[\"AMORTISATION_COLUMNS\"]\n",
    ")\n",
    "print(\"Cast data to correct types.\")\n",
    "final_df = (\n",
    "    unpivot_df.withColumn(\"tmp_col_name\", F.to_date(F.col(\"DATE_VALUE\")))\n",
    "    .drop(\"DATE_VALUE\")\n",
    "    .withColumnRenamed(\"tmp_col_name\", \"DATE_VALUE\")\n",
    "    .withColumn(\n",
    "        \"tmp_col_name\", F.round(F.col(\"DOUBLE_VALUE\").cast(DoubleType()), 2)\n",
    "    )\n",
    "    .drop(\"DOUBLE_VALUE\")\n",
    "    .withColumnRenamed(\"tmp_col_name\", \"DOUBLE_VALUE\")\n",
    "    .withColumn(\"year\", F.year(F.col(\"DATE_VALUE\")))\n",
    "    .withColumn(\"month\", F.month(F.col(\"DATE_VALUE\")))\n",
    "    .withColumn(\"day\", F.dayofmonth(F.col(\"DATE_VALUE\")))\n",
    ")\n",
    "(\n",
    "    final_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../dataoutput/bronze/amortisation.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEAL DETAILS BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Deal_Details\"\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/{file_key}/*.xml\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files[0]\n",
    "\n",
    "\n",
    "def create_dataframe(deal_detail_file):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param deal_detail_file: file to be read to generate the dataframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    xml_data = objectify.parse(deal_detail_file)  # Parse XML data\n",
    "    root = xml_data.getroot()  # Root element\n",
    "\n",
    "    data = []\n",
    "    cols = []\n",
    "    for i in range(\n",
    "        len(\n",
    "            root.getchildren()[1]\n",
    "            .getchildren()[0]\n",
    "            .getchildren()[1]\n",
    "            .getchildren()[0]\n",
    "            .getchildren()\n",
    "        )\n",
    "    ):\n",
    "        child = (\n",
    "            root.getchildren()[1]\n",
    "            .getchildren()[0]\n",
    "            .getchildren()[1]\n",
    "            .getchildren()[0]\n",
    "            .getchildren()[i]\n",
    "        )\n",
    "        data.append(child.text)\n",
    "        cols.append(child.tag.replace(\"{http://edwin.eurodw.eu/EDServices/2.3}\", \"\"))\n",
    "\n",
    "    df = pd.DataFrame(data).T  # Create DataFrame and transpose it\n",
    "    df.columns = cols  # Update column names\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Start DEAL DETAILS BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "xml_file = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "final_df = create_dataframe(xml_file)\n",
    "(\n",
    "    final_df.format(\"parquet\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../dataoutput/bronze/deal_details/info.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSET SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\n",
    "        \"AS1\",\n",
    "        \"AS19\",\n",
    "        \"AS20\",\n",
    "        \"AS31\",\n",
    "        \"AS50\",\n",
    "        \"AS51\",\n",
    "        \"AS67\",\n",
    "        \"AS70\",\n",
    "        \"AS71\",\n",
    "        \"AS87\",\n",
    "        \"AS91\",\n",
    "        \"AS112\",\n",
    "        \"AS124\",\n",
    "        \"AS127\",\n",
    "        \"AS130\",\n",
    "        \"AS133\",\n",
    "        \"AS134\",\n",
    "        \"AS137\",\n",
    "    ]\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_columns_collection(df):\n",
    "    \"\"\"\n",
    "    Get collection of dataframe columns divided by topic.\n",
    "\n",
    "    :param df: Asset bronze Spark dataframe.\n",
    "    :return cols_dict: collection of columns labelled by topic.\n",
    "    \"\"\"\n",
    "    cols_dict = {\n",
    "        \"general\": [\"ID\", \"year\", \"month\", \"day\"]\n",
    "        + [f\"AS{i}\" for i in range(1, 15) if f\"AS{i}\" in df.columns],\n",
    "        \"obligor_info\": [f\"AS{i}\" for i in range(15, 50) if f\"AS{i}\" in df.columns],\n",
    "        \"loan_info\": [f\"AS{i}\" for i in range(50, 80) if f\"AS{i}\" in df.columns],\n",
    "        \"interest_rate\": [f\"AS{i}\" for i in range(80, 100) if f\"AS{i}\" in df.columns],\n",
    "        \"financial_info\": [f\"AS{i}\" for i in range(100, 115) if f\"AS{i}\" in df.columns],\n",
    "        \"performance_info\": [\n",
    "            f\"AS{i}\" for i in range(115, 146) if f\"AS{i}\" in df.columns\n",
    "        ],\n",
    "    }\n",
    "    return cols_dict\n",
    "\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_obligor_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract obligor info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"obligor_info\"])\n",
    "        .withColumn(\"tmp_AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "        .withColumnRenamed(\"tmp_AS1\", \"AS1\")\n",
    "        .withColumn(\"tmp_AS19\", F.unix_timestamp(F.col(\"AS19\")))\n",
    "        .drop(\"AS19\")\n",
    "        .withColumnRenamed(\"tmp_AS19\", \"AS19\")\n",
    "        .withColumn(\"tmp_AS20\", F.unix_timestamp(F.col(\"AS20\")))\n",
    "        .drop(\"AS20\")\n",
    "        .withColumnRenamed(\"tmp_AS20\", \"AS20\")\n",
    "        .withColumn(\"tmp_AS31\", F.unix_timestamp(F.col(\"AS31\")))\n",
    "        .drop(\"AS31\")\n",
    "        .withColumnRenamed(\"tmp_AS31\", \"AS31\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_loan_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract loan info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"loan_info\"])\n",
    "        .withColumn(\"tmp_AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "        .withColumnRenamed(\"tmp_AS1\", \"AS1\")\n",
    "        .withColumn(\"tmp_AS50\", F.unix_timestamp(F.col(\"AS50\")))\n",
    "        .drop(\"AS50\")\n",
    "        .withColumnRenamed(\"tmp_AS50\", \"AS50\")\n",
    "        .withColumn(\"tmp_AS51\", F.unix_timestamp(F.col(\"AS51\")))\n",
    "        .drop(\"AS51\")\n",
    "        .withColumnRenamed(\"tmp_AS51\", \"AS51\")\n",
    "        .withColumn(\"tmp_AS67\", F.unix_timestamp(F.col(\"AS67\")))\n",
    "        .drop(\"AS67\")\n",
    "        .withColumnRenamed(\"tmp_AS67\", \"AS67\")\n",
    "        .withColumn(\"tmp_AS70\", F.unix_timestamp(F.col(\"AS70\")))\n",
    "        .drop(\"AS70\")\n",
    "        .withColumnRenamed(\"tmp_AS70\", \"AS70\")\n",
    "        .withColumn(\"tmp_AS71\", F.unix_timestamp(F.col(\"AS71\")))\n",
    "        .drop(\"AS71\")\n",
    "        .withColumnRenamed(\"tmp_AS71\", \"AS71\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_interest_rate(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract interest rate dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"interest_rate\"])\n",
    "        .withColumn(\"tmp_AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "        .withColumnRenamed(\"tmp_AS1\", \"AS1\")\n",
    "        .withColumn(\"tmp_AS87\", F.unix_timestamp(F.col(\"AS87\")))\n",
    "        .drop(\"AS87\")\n",
    "        .withColumnRenamed(\"tmp_AS87\", \"AS87\")\n",
    "        .withColumn(\"tmp_AS91\", F.unix_timestamp(F.col(\"AS91\")))\n",
    "        .drop(\"AS91\")\n",
    "        .withColumnRenamed(\"tmp_AS91\", \"AS91\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_financial_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract financial info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"financial_info\"])\n",
    "        .withColumn(\"tmp_AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "        .withColumnRenamed(\"tmp_AS1\", \"AS1\")\n",
    "        .withColumn(\"tmp_AS112\", F.unix_timestamp(F.col(\"AS112\")))\n",
    "        .drop(\"AS112\")\n",
    "        .withColumnRenamed(\"tmp_AS112\", \"AS112\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_performance_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract performance info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"general\"] + cols_dict[\"performance_info\"])\n",
    "        .withColumn(\"tmp_AS1\", F.unix_timestamp(F.col(\"AS1\")))\n",
    "        .drop(\"AS1\")\n",
    "        .withColumnRenamed(\"tmp_AS1\", \"AS1\")\n",
    "        .withColumn(\"tmp_AS124\", F.unix_timestamp(F.col(\"AS124\")))\n",
    "        .drop(\"AS124\")\n",
    "        .withColumnRenamed(\"tmp_AS124\", \"AS124\")\n",
    "        .withColumn(\"tmp_AS127\", F.unix_timestamp(F.col(\"AS127\")))\n",
    "        .drop(\"AS127\")\n",
    "        .withColumnRenamed(\"tmp_AS127\", \"AS127\")\n",
    "        .withColumn(\"tmp_AS130\", F.unix_timestamp(F.col(\"AS130\")))\n",
    "        .drop(\"AS130\")\n",
    "        .withColumnRenamed(\"tmp_AS130\", \"AS130\")\n",
    "        .withColumn(\"tmp_AS133\", F.unix_timestamp(F.col(\"AS133\")))\n",
    "        .drop(\"AS133\")\n",
    "        .withColumnRenamed(\"tmp_AS133\", \"AS133\")\n",
    "        .withColumn(\"tmp_AS134\", F.unix_timestamp(F.col(\"AS134\")))\n",
    "        .drop(\"AS134\")\n",
    "        .withColumnRenamed(\"tmp_AS134\", \"AS134\")\n",
    "        .withColumn(\"tmp_AS137\", F.unix_timestamp(F.col(\"AS137\")))\n",
    "        .drop(\"AS137\")\n",
    "        .withColumnRenamed(\"tmp_AS137\", \"AS137\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/bronze/assets.parquet'\n",
    ")\n",
    "assets_columns = get_columns_collection(bronze_df)\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(bronze_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate obligor info dataframe\")\n",
    "obligor_info_df = process_obligor_info(bronze_df, assets_columns)\n",
    "print(\"Generate loan info dataframe\")\n",
    "loan_info_df = process_loan_info(bronze_df, assets_columns)\n",
    "print(\"Generate interest rate dataframe\")\n",
    "interest_rate_df = process_interest_rate(bronze_df, assets_columns)\n",
    "print(\"Generate financial info dataframe\")\n",
    "financial_info_df = process_financial_info(bronze_df, assets_columns)\n",
    "print(\"Generate performace info dataframe\")\n",
    "performance_info_df = process_performance_info(bronze_df, assets_columns)\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    date_df.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/date_table.parquet\")\n",
    ")\n",
    "(\n",
    "    loan_info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/loan_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    obligor_info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/obligor_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    financial_info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/financial_info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    interest_rate_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/interest_rate_table.parquet\")\n",
    ")\n",
    "(\n",
    "    performance_info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/assets/performance_info_table.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMORTISATION SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    return config\n",
    "\n",
    "\n",
    "def process_dates(df):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(\"DATE_VALUE\")\n",
    "        .alias(\"date_col\")\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_info(df):\n",
    "    \"\"\"\n",
    "    Extract amortisation values dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = df.withColumn(\n",
    "        \"tmp_DATE_VALUE\", F.unix_timestamp(F.col(\"DATE_VALUE\"))\n",
    "    ).drop(\"DATE_VALUE\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start AMORTISATION SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/bronze/amortisation.parquet'\n",
    ")\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(bronze_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate info dataframe\")\n",
    "info_df = process_info(bronze_df)\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    date_df.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/amortisation/date_table.parquet\")\n",
    ")\n",
    "(\n",
    "    info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/amortisation/info_table.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND INFO SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\"BS1\"]\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_columns_collection(df):\n",
    "    \"\"\"\n",
    "    Get collection of dataframe columns divided by topic.\n",
    "\n",
    "    :param df: Asset bronze Spark dataframe.\n",
    "    :return cols_dict: collection of columns labelled by topic.\n",
    "    \"\"\"\n",
    "    cols_dict = {\n",
    "        \"bond_info\": [\"ID\", \"year\", \"month\", \"day\"]\n",
    "        + [f\"BS{i}\" for i in range(1, 11) if f\"BS{i}\" in df.columns],\n",
    "        \"collateral_info\": [\"ID\", \"year\", \"month\", \"day\"]\n",
    "        + [f\"BS{i}\" for i in range(11, 19) if f\"BS{i}\" in df.columns],\n",
    "        \"contact_info\": [\"ID\", \"year\", \"month\", \"day\"]\n",
    "        + [f\"BS{i}\" for i in range(19, 25) if f\"BS{i}\" in df.columns],\n",
    "    }\n",
    "    return cols_dict\n",
    "\n",
    "\n",
    "def process_dates(df, col_types_dict):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param col_types_dict: collection of columns and their types.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in col_types_dict[\"date\"] if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_bond_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract bond info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select(cols_dict[\"bond_info\"])\n",
    "        .withColumn(\n",
    "            \"tmp_BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .drop(\"BS1\")\n",
    "        .withColumnRenamed(\"tmp_BS1\", \"BS1\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_collateral_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract collateral info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select([\"BS1\", \"BS2\"] + cols_dict[\"collateral_info\"])\n",
    "        .withColumn(\n",
    "            \"tmp_BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .drop(\"BS1\")\n",
    "        .withColumnRenamed(\"tmp_BS1\", \"BS1\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_contact_info(df, cols_dict):\n",
    "    \"\"\"\n",
    "    Extract contact info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param cols_dict: collection of columns labelled by their topic.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.select([\"BS1\", \"BS2\"] + cols_dict[\"contact_info\"])\n",
    "        .withColumn(\n",
    "            \"tmp_BS1\", F.unix_timestamp(F.to_timestamp(F.col(\"BS1\"), \"yyyy-MM-dd\"))\n",
    "        )\n",
    "        .drop(\"BS1\")\n",
    "        .withColumnRenamed(\"tmp_BS1\", \"BS1\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start BOND INFO SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/bronze/bond_info.parquet'\n",
    ")\n",
    "bond_info_columns = get_columns_collection(bronze_df)\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(bronze_df, run_props[\"DATE_COLUMNS\"])\n",
    "print(\"Generate bond info dataframe\")\n",
    "info_df = process_bond_info(bronze_df, bond_info_columns)\n",
    "print(\"Generate collateral info dataframe\")\n",
    "collateral_df = process_collateral_info(bronze_df, bond_info_columns)\n",
    "print(\"Generate contact info dataframe\")\n",
    "contact_df = process_contact_info(bronze_df, bond_info_columns)\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    date_df.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/bond_info/date_table.parquet\")\n",
    ")\n",
    "(\n",
    "    info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/bond_info/info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    collateral_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/bond_info/collaterals_table.parquet\")\n",
    ")\n",
    "(\n",
    "    contact_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/bond_info/contacts_table.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLATERAL SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = SILVER_SOURCE_DIR\n",
    "    config[\"DATE_COLUMNS\"] = [\"CS11\", \"CS12\", \"CS22\"]\n",
    "    return config\n",
    "\n",
    "\n",
    "def process_dates(df, date_cols_list):\n",
    "    \"\"\"\n",
    "    Extract dates dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :param date_cols_list: list of date columns.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    date_cols = [c for c in date_cols_list if c in df.columns]\n",
    "\n",
    "    new_df = (\n",
    "        df.select(F.explode(F.array(date_cols)).alias(\"date_col\"))\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"unix_date\", F.unix_timestamp(F.col(\"date_col\")))\n",
    "        .withColumn(\"year\", F.year(F.col(\"date_col\")))\n",
    "        .withColumn(\"month\", F.month(F.col(\"date_col\")))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(\"date_col\")))\n",
    "        .withColumn(\"WoY\", F.weekofyear(F.col(\"date_col\")))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(\"date_col\")))\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def process_collateral_info(df):\n",
    "    \"\"\"\n",
    "    Extract collateral info dimension from bronze Spark dataframe.\n",
    "\n",
    "    :param df: Spark bronze dataframe.\n",
    "    :return new_df: silver type Spark dataframe.\n",
    "    \"\"\"\n",
    "    new_df = (\n",
    "        df.withColumn(\n",
    "            \"tmp_CS11\", F.unix_timestamp(F.to_timestamp(F.col(\"CS11\"), \"yyyy-MM\"))\n",
    "        )\n",
    "        .drop(\"CS11\")\n",
    "        .withColumnRenamed(\"tmp_CS11\", \"CS11\")\n",
    "        .withColumn(\n",
    "            \"tmp_CS12\", F.unix_timestamp(F.to_timestamp(F.col(\"CS12\"), \"yyyy-MM\"))\n",
    "        )\n",
    "        .drop(\"CS12\")\n",
    "        .withColumnRenamed(\"tmp_CS12\", \"CS12\")\n",
    "        .withColumn(\n",
    "            \"tmp_CS22\", F.unix_timestamp(F.to_timestamp(F.col(\"CS22\"), \"yyyy-MM\"))\n",
    "        )\n",
    "        .drop(\"CS22\")\n",
    "        .withColumnRenamed(\"tmp_CS22\", \"CS22\")\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "print(\"Start COLLATERAL SILVER job.\")\n",
    "run_props = set_job_params()\n",
    "bronze_df = SPARK.read.parquet(\n",
    "    f'{run_props[\"SOURCE_DIR\"]}/bronze/collaterals.parquet'\n",
    ")\n",
    "print(\"Generate collateral info dataframe\")\n",
    "info_df = process_collateral_info(bronze_df)\n",
    "print(\"Generate time dataframe\")\n",
    "date_df = process_dates(bronze_df, run_props[\"DATE_COLUMNS\"])\n",
    "\n",
    "print(\"Write dataframe\")\n",
    "\n",
    "(\n",
    "    info_df.format(\"parquet\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/collaterals/info_table.parquet\")\n",
    ")\n",
    "(\n",
    "    date_df.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../dataoutput/silver/collaterals/date_table.parquet\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
