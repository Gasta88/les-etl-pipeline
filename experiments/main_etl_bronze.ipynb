{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main ETL for Loan Level Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import sys\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import DateType, StringType, DoubleType, BooleanType, TimestampType\n",
        "import csv\n",
        "from functools import reduce\n",
        "from lxml import objectify\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "SPARK = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "BRONZE_SOURCE_DIR = \"../data/mini_source\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ASSET BRONZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start ASSETS BRONZE job.\n",
            "Retrieved 19 asset data files.\n"
          ]
        }
      ],
      "source": [
        "def set_job_params():\n",
        "    \"\"\"\n",
        "    Setup parameters used for this module.\n",
        "\n",
        "    :return config: dictionary with properties used in this job.\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
        "    config[\"FILE_KEY\"] = \"Loan_Data\"\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_raw_files(source_dir, file_key):\n",
        "    \"\"\"\n",
        "    Return list of files that satisfy the file_key parameter.\n",
        "    Works only on local machine so far.\n",
        "\n",
        "    :param source_dir: folder path where files are stored.\n",
        "    :param file_key: label for file name that helps with the cherry picking.\n",
        "    :return all_files: list of desired files from source_dir.\n",
        "    \"\"\"\n",
        "    all_files = [\n",
        "        f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\") if \"Labeled0M\" not in f\n",
        "    ]\n",
        "    if len(all_files) == 0:\n",
        "        print(\n",
        "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return all_files\n",
        "\n",
        "\n",
        "def create_source_dataframe(spark, all_files):\n",
        "    \"\"\"\n",
        "    Read files and generate one PySpark DataFrame from them.\n",
        "\n",
        "    :param spark: SparkSession object.\n",
        "    :param all_files: list of files to be read to generate the dtaframe.\n",
        "    :return df: PySpark datafram for loan asset data.\n",
        "    \"\"\"\n",
        "    list_dfs = []\n",
        "    for csv_f in all_files:\n",
        "        col_names = []\n",
        "        content = []\n",
        "        with open(csv_f, \"r\") as f:\n",
        "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
        "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
        "            for i, line in enumerate(csv.reader(f)):\n",
        "                if i == 0:\n",
        "                    col_names = line\n",
        "                    col_names[0] = \"AS1\"\n",
        "                elif i == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(line)\n",
        "            df = (\n",
        "                    spark.createDataFrame(content, col_names)\n",
        "                    .withColumn(\"ed_code\", F.lit(csv_id))\n",
        "                    .replace(\"\",None)\n",
        "                    .withColumn(\"ImportDate\", F.lit(csv_date))\n",
        "                    .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
        "                    .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
        "                    .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
        "                    .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
        "                    .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
        "                    .withColumn(\"checksum\",F.md5(F.concat(F.col(\"ed_code\"),\n",
        "                                                          F.col(\"AS1\"),\n",
        "                                                          F.col(\"AS2\"),\n",
        "                                                          F.col(\"AS3\"),\n",
        "                                                          F.col(\"AS4\"),\n",
        "                                                          F.col(\"AS5\"),\n",
        "                                                          F.col(\"AS6\"),\n",
        "                                                          F.col(\"AS7\"))))\n",
        "                    .drop(\"ImportDate\")\n",
        "                )\n",
        "            list_dfs.append(df)\n",
        "    if list_dfs == []:\n",
        "        print(\"No dataframes were extracted from files. Exit process!\")\n",
        "        sys.exit(1)\n",
        "    return reduce(DataFrame.union, list_dfs)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Start ASSETS BRONZE job.\")\n",
        "run_props = set_job_params()\n",
        "all_asset_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
        "print(f\"Retrieved {len(all_asset_files)} asset data files.\")\n",
        "raw_asset_df = create_source_dataframe(SPARK, all_asset_files)\n",
        "(\n",
        "    raw_asset_df.write\n",
        "    .partitionBy(\"year\", \"month\")\n",
        "    .mode(\"append\")\n",
        "    .parquet(\"../data/output/bronze/assets.parquet\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COLLATERAL BRONZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start COLLATERAL BRONZE job.\n",
            "Retrieved 19 collateral data files.\n"
          ]
        }
      ],
      "source": [
        "def set_job_params():\n",
        "    \"\"\"\n",
        "    Setup parameters used for this module.\n",
        "\n",
        "    :return config: dictionary with properties used in this job.\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
        "    config[\"FILE_KEY\"] = \"Collateral\"\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_raw_files(source_dir, file_key):\n",
        "    \"\"\"\n",
        "    Return list of files that satisfy the file_key parameter.\n",
        "    Works only on local machine so far.\n",
        "\n",
        "    :param source_dir: folder path where files are stored.\n",
        "    :param file_key: label for file name that helps with the cherry picking.\n",
        "    :return all_files: listof desired files from source_dir.\n",
        "    \"\"\"\n",
        "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
        "    if len(all_files) == 0:\n",
        "        print(\n",
        "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return all_files\n",
        "\n",
        "\n",
        "def create_source_dataframe(spark, all_files):\n",
        "    \"\"\"\n",
        "    Read files and generate one PySpark DataFrame from them.\n",
        "\n",
        "    :param spark: SparkSession object.\n",
        "    :param all_files: list of files to be read to generate the dtaframe.\n",
        "    :return df: PySpark dataframe for loan asset data.\n",
        "    \"\"\"\n",
        "    list_dfs = []\n",
        "    for csv_f in all_files:\n",
        "        col_names = []\n",
        "        content = []\n",
        "        with open(csv_f, \"r\") as f:\n",
        "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
        "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
        "            for i, line in enumerate(csv.reader(f)):\n",
        "                if i == 0:\n",
        "                    col_names = line\n",
        "                    col_names[0] = \"CS1\"\n",
        "                elif i == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(line)\n",
        "            df = (\n",
        "                spark.createDataFrame(content, col_names)\n",
        "                .withColumn(\"ed_code\", F.lit(csv_id))\n",
        "                .replace(\"\",None)\n",
        "                .withColumn(\"ImportDate\", F.lit(csv_date))\n",
        "                .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
        "                .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
        "                .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
        "                .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
        "                .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
        "                .withColumn(\"checksum\",F.md5(F.concat(F.col(\"ed_code\"),\n",
        "                                                      F.col(\"CS1\"),\n",
        "                                                      F.col(\"CS2\"))))\n",
        "                .drop(\"ImportDate\")\n",
        "            )\n",
        "            list_dfs.append(df)\n",
        "    if list_dfs == []:\n",
        "        print(\"No dataframes were extracted from files. Exit process!\")\n",
        "        sys.exit(1)\n",
        "    return reduce(DataFrame.union, list_dfs)\n",
        "\n",
        "print(\"Start COLLATERAL BRONZE job.\")\n",
        "run_props = set_job_params()\n",
        "all_collateral_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
        "print(f\"Retrieved {len(all_collateral_files)} collateral data files.\")\n",
        "raw_collateral_df = create_source_dataframe(SPARK, all_collateral_files)\n",
        "(\n",
        "    raw_collateral_df.write\n",
        "    .partitionBy(\"year\", \"month\")\n",
        "    .mode(\"append\")\n",
        "    .parquet(\"../data/output/bronze/collaterals.parquet\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BOND INFO BRONZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start BOND INFO BRONZE job.\n",
            "Retrieved 19 bond info data files.\n"
          ]
        }
      ],
      "source": [
        "def set_job_params():\n",
        "    \"\"\"\n",
        "    Setup parameters used for this module.\n",
        "\n",
        "    :return config: dictionary with properties used in this job.\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
        "    config[\"FILE_KEY\"] = \"Bond_Info\"\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_raw_files(source_dir, file_key):\n",
        "    \"\"\"\n",
        "    Return list of files that satisfy the file_key parameter.\n",
        "    Works only on local machine so far.\n",
        "\n",
        "    :param source_dir: folder path where files are stored.\n",
        "    :param file_key: label for file name that helps with the cherry picking.\n",
        "    :return all_files: listof desired files from source_dir.\n",
        "    \"\"\"\n",
        "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
        "    if len(all_files) == 0:\n",
        "        print(\n",
        "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return all_files\n",
        "\n",
        "\n",
        "def create_source_dataframe(spark, all_files):\n",
        "    \"\"\"\n",
        "    Read files and generate one PySpark DataFrame from them.\n",
        "\n",
        "    :param spark: SparkSession object.\n",
        "    :param all_files: list of files to be read to generate the dtaframe.\n",
        "    :return df: PySpark datafram for loan asset data.\n",
        "    \"\"\"\n",
        "    list_dfs = []\n",
        "    for csv_f in all_files:\n",
        "        col_names = []\n",
        "        content = []\n",
        "        with open(csv_f, \"r\") as f:\n",
        "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
        "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
        "            for i, line in enumerate(csv.reader(f)):\n",
        "                if i == 0:\n",
        "                    col_names = line\n",
        "                    col_names[0] = \"BS1\"\n",
        "                elif i == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(line)\n",
        "            df = (\n",
        "                spark.createDataFrame(content, col_names)\n",
        "                .withColumn(\"ed_code\", F.lit(csv_id))\n",
        "                .replace(\"\",None)\n",
        "                .withColumn(\"ImportDate\", F.lit(csv_date))\n",
        "                .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
        "                .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
        "                .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
        "                .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
        "                .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
        "                .withColumn(\"checksum\",F.md5(F.concat(F.col(\"ed_code\"), \n",
        "                                                      F.col(\"BS1\"),\n",
        "                                                      F.col(\"BS2\"))))\n",
        "                .drop(\"ImportDate\")\n",
        "            )\n",
        "            list_dfs.append(df)\n",
        "    if list_dfs == []:\n",
        "        print(\"No dataframes were extracted from files. Exit process!\")\n",
        "        sys.exit(1)\n",
        "    return reduce(DataFrame.union, list_dfs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Start BOND INFO BRONZE job.\")\n",
        "run_props = set_job_params()\n",
        "all_bond_info_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
        "print(f\"Retrieved {len(all_bond_info_files)} bond info data files.\")\n",
        "raw_bond_info_df = create_source_dataframe(SPARK, all_bond_info_files)\n",
        "(\n",
        "    raw_bond_info_df.write\n",
        "    .partitionBy(\"year\", \"month\")\n",
        "    .mode(\"append\")\n",
        "    .parquet(\"../data/output/bronze/bond_info.parquet\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AMORTISATION BRONZE\n",
        "\n",
        "To be reviewed since even one portfolio will bring it OutOfMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start AMORTISATION BRONZE job.\n",
            "Retrieved 19 amortisation data files.\n"
          ]
        }
      ],
      "source": [
        "def set_job_params():\n",
        "    \"\"\"\n",
        "    Setup parameters used for this module.\n",
        "\n",
        "    :return config: dictionary with properties used in this job.\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
        "    config[\"FILE_KEY\"] = \"Amortization\"\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_raw_files(source_dir, file_key):\n",
        "    \"\"\"\n",
        "    Return list of files that satisfy the file_key parameter.\n",
        "    Works only on local machine so far.\n",
        "\n",
        "    :param source_dir: folder path where files are stored.\n",
        "    :param file_key: label for file name that helps with the cherry picking.\n",
        "    :return all_files: listof desired files from source_dir.\n",
        "    \"\"\"\n",
        "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
        "    if len(all_files) == 0:\n",
        "        print(\n",
        "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return all_files\n",
        "\n",
        "\n",
        "def create_source_dataframe(spark, all_files):\n",
        "    \"\"\"\n",
        "    Read files and generate one PySpark DataFrame from them.\n",
        "\n",
        "    :param spark: SparkSession object.\n",
        "    :param all_files: list of files to be read to generate the dtaframe.\n",
        "    :return df: PySpark dataframe for loan asset data.\n",
        "    \"\"\"\n",
        "    list_dfs = []\n",
        "    for csv_f in all_files:\n",
        "        col_names = []\n",
        "        content = []\n",
        "        with open(csv_f, \"r\") as f:\n",
        "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
        "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
        "            for i, line in enumerate(csv.reader(f)):\n",
        "                if i == 0:\n",
        "                    col_names = line\n",
        "                    col_names[0] = \"AS3\" #visible error from EDW where first columns has different names\n",
        "                elif i == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(line) # fix empty \"\" because they are messy\n",
        "            df = (\n",
        "                    spark.createDataFrame(content, col_names)\n",
        "                    .withColumn(\"ed_code\", F.lit(csv_id))\n",
        "                    .replace(\"\",None)\n",
        "                    .withColumn(\"ImportDate\", F.lit(csv_date))\n",
        "                    .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
        "                    .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
        "                    .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
        "                    .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
        "                    .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
        "                    .withColumn(\"checksum\",F.md5(F.concat(F.col(\"ed_code\"),\n",
        "                                                 F.col(\"AS3\"))))\n",
        "                    .drop(\"ImportDate\")\n",
        "                )\n",
        "            list_dfs.append(df)\n",
        "    return reduce(DataFrame.union, list_dfs)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Start AMORTISATION BRONZE job.\")\n",
        "run_props = set_job_params()\n",
        "all_amortisation_files = get_raw_files(\n",
        "    run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"]\n",
        ")\n",
        "print(f\"Retrieved {len(all_amortisation_files)} amortisation data files.\")\n",
        "raw_amortisation_df = (\n",
        "            create_source_dataframe(SPARK, all_amortisation_files[:1])\n",
        "        )\n",
        "\n",
        "(\n",
        "    raw_amortisation_df.write\n",
        "    .partitionBy(\"year\", \"month\")\n",
        "    .mode(\"append\")\n",
        "    .parquet(\"../data/output/bronze/amortisation.parquet\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DEAL DETAILS BRONZE\n",
        "To be reviewed since it is unclear how to store data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start DEAL DETAILS BRONZE job.\n"
          ]
        }
      ],
      "source": [
        "def set_job_params():\n",
        "    \"\"\"\n",
        "    Setup parameters used for this module.\n",
        "\n",
        "    :return config: dictionary with properties used in this job.\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
        "    config[\"FILE_KEY\"] = \"Deal_Details\"\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_raw_files(source_dir, file_key):\n",
        "    \"\"\"\n",
        "    Return list of files that satisfy the file_key parameter.\n",
        "    Works only on local machine so far.\n",
        "\n",
        "    :param source_dir: folder path where files are stored.\n",
        "    :param file_key: label for file name that helps with the cherry picking.\n",
        "    :return all_files: listof desired files from source_dir.\n",
        "    \"\"\"\n",
        "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.xml\")]\n",
        "    if len(all_files) == 0:\n",
        "        print(\n",
        "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return all_files\n",
        "\n",
        "\n",
        "def create_source_dataframe(deal_detail_files):\n",
        "    \"\"\"\n",
        "    Read files and generate one PySpark DataFrame from them.\n",
        "\n",
        "    :param deal_detail_files: files to be read to generate the dataframe.\n",
        "    :return df: PySpark datafram for loan asset data.\n",
        "    \"\"\"\n",
        "    list_dfs = []\n",
        "    for xml_f in deal_detail_files:\n",
        "        xml_data = objectify.parse(xml_f)  # Parse XML data\n",
        "        root = xml_data.getroot()  # Root element\n",
        "\n",
        "        data = []\n",
        "        cols = []\n",
        "        for i in range(\n",
        "            len(\n",
        "                root.getchildren()[1]\n",
        "                .getchildren()[0]\n",
        "                .getchildren()[1]\n",
        "                .getchildren()[0]\n",
        "                .getchildren()\n",
        "            )\n",
        "        ):\n",
        "            child = (\n",
        "                root.getchildren()[1]\n",
        "                .getchildren()[0]\n",
        "                .getchildren()[1]\n",
        "                .getchildren()[0]\n",
        "                .getchildren()[i]\n",
        "            )\n",
        "            tag = child.tag.replace(\"{http://edwin.eurodw.eu/EDServices/2.3}\", \"\")\n",
        "            if tag == \"ISIN\":\n",
        "                # is array\n",
        "                data.append(\";\".join(map(str,child.getchildren())))\n",
        "            elif tag in [\"Country\", \"DealVisibleToOrg\", \"DealVisibleToUser\", \"Submissions\"]:\n",
        "                # usually null values\n",
        "                #TODO: Submissions might have interesting stuff. Ask to Luca.\n",
        "                continue\n",
        "            else:\n",
        "                data.append(child.text)\n",
        "            cols.append(tag)\n",
        "\n",
        "        df = pd.DataFrame(data).T  # Create DataFrame and transpose it\n",
        "        df.columns = cols  # Update column names\n",
        "        df[\"valid_from\"] = pd.Timestamp.now() \n",
        "        df[\"valid_to\"] = None\n",
        "        df[\"iscurrent\"] = 1\n",
        "        df[\"checksum\"] =pd.util.hash_pandas_object(df[\"EDCode\"]) # Check if you can add \"year\" and \"month\"\n",
        "        list_dfs.append(df)\n",
        "    \n",
        "    return pd.concat(list_dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "print(\"Start DEAL DETAILS BRONZE job.\")\n",
        "run_props = set_job_params()\n",
        "all_xml_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
        "raw_deal_details_df = create_source_dataframe(all_xml_files)\n",
        "\n",
        "(\n",
        "    raw_deal_details_df\n",
        "    .to_csv(\n",
        "        \"../data/output/bronze/deal_details.csv\", \n",
        "        mode='a', \n",
        "        header=not os.path.exists(\"../data/output/bronze/deal_details.csv\"\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
