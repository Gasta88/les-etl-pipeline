{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main ETL for Loan Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, StringType, DoubleType, BooleanType, TimestampType\n",
    "import csv\n",
    "from functools import reduce\n",
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "SPARK = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "BRONZE_SOURCE_DIR = \"../data/mini_source\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSET BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Loan_Data\"\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: list of desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [\n",
    "        f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\") if \"Labeled0M\" not in f\n",
    "    ]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
    "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                    col_names[0] = \"AS1\"\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = (\n",
    "                    spark.createDataFrame(content, col_names)\n",
    "                    .withColumn(\"ed_code\", F.lit(csv_id))\n",
    "                    .replace(\"\",None)\n",
    "                    .withColumn(\"ImportDate\", F.lit(csv_date))\n",
    "                    .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
    "                    .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
    "                    .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
    "                    .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
    "                    .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
    "                    .withColumn(\"checksum\",F.md5(F.concat(F.col(\"AS1\"),F.col(\"AS2\"),F.col(\"AS3\"),F.col(\"AS4\"),F.col(\"AS5\"),F.col(\"AS6\"),F.col(\"AS7\"),)))\n",
    "                    .drop(\"ImportDate\")\n",
    "                )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start ASSETS BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_asset_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_asset_files)} asset data files.\")\n",
    "raw_asset_df = create_dataframe(SPARK, all_asset_files)\n",
    "(\n",
    "    raw_asset_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"../data/output/bronze/assets.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLATERAL BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Collateral\"\n",
    "    # config[\"COLLATERAL_COLUMNS\"] = {\n",
    "    #     \"CS1\": StringType(),\n",
    "    #     \"CS2\": StringType(),\n",
    "    #     \"CS3\": StringType(),\n",
    "    #     \"CS4\": DoubleType(),\n",
    "    #     \"CS5\": DoubleType(),\n",
    "    #     \"CS6\": StringType(),\n",
    "    #     \"CS7\": BooleanType(),\n",
    "    #     \"CS8\": BooleanType(),\n",
    "    #     \"CS9\": BooleanType(),\n",
    "    #     \"CS10\": DoubleType(),\n",
    "    #     \"CS11\": DateType(),\n",
    "    #     \"CS12\": DateType(),\n",
    "    #     \"CS13\": StringType(),\n",
    "    #     \"CS14\": StringType(),\n",
    "    #     \"CS15\": DoubleType(),\n",
    "    #     \"CS16\": StringType(),\n",
    "    #     \"CS17\": StringType(),\n",
    "    #     \"CS18\": DoubleType(),\n",
    "    #     \"CS19\": DoubleType(),\n",
    "    #     \"CS20\": StringType(),\n",
    "    #     \"CS21\": DoubleType(),\n",
    "    #     \"CS22\": DateType(),\n",
    "    #     \"CS23\": StringType(),\n",
    "    #     \"CS24\": StringType(),\n",
    "    #     \"CS25\": StringType(),\n",
    "    #     \"CS26\": StringType(),\n",
    "    #     \"CS27\": StringType(),\n",
    "    #     \"CS28\": DoubleType(),\n",
    "    # }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark dataframe for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
    "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                    col_names[0] = \"CS1\"\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = (\n",
    "                spark.createDataFrame(content, col_names)\n",
    "                .withColumn(\"ed_code\", F.lit(csv_id))\n",
    "                .replace(\"\",None)\n",
    "                .withColumn(\"ImportDate\", F.lit(csv_date))\n",
    "                .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
    "                .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
    "                .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
    "                .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
    "                .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
    "                .withColumn(\"checksum\",F.md5(F.concat(F.col(\"CS1\"),F.col(\"CS2\"))))\n",
    "                .drop(\"ImportDate\")\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "# def replace_no_data(df):\n",
    "#     \"\"\"\n",
    "#     Replace ND values inside the dataframe\n",
    "#     TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "#           Should handle this information better in future releases.\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :return df: Spark dataframe without ND values.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         df = df.withColumn(\n",
    "#             col_name,\n",
    "#             F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "#         )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def replace_bool_data(df):\n",
    "#     \"\"\"\n",
    "#     Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :return df: Spark dataframe without Y/N values.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         df = df.withColumn(\n",
    "#             col_name,\n",
    "#             F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "#             .when(F.col(col_name) == \"N\", \"False\")\n",
    "#             .otherwise(F.col(col_name)),\n",
    "#         )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def cast_to_datatype(df, columns):\n",
    "#     \"\"\"\n",
    "#     Cast data to the respective datatype.\n",
    "\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :param columns: collection of column names and respective data types.\n",
    "#     :return df: Spark dataframe with correct values.\n",
    "#     \"\"\"\n",
    "#     for col_name, data_type in columns.items():\n",
    "#         if data_type == BooleanType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\"tmp_col_name\", F.col(col_name).contains(\"True\"))\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#         if data_type == DateType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\"tmp_col_name\", F.to_date(F.col(col_name)))\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#         if data_type == DoubleType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\n",
    "#                     \"tmp_col_name\", F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "#                 )\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#     return df\n",
    "\n",
    "\n",
    "print(\"Start COLLATERAL BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_collateral_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_collateral_files)} collateral data files.\")\n",
    "raw_collateral_df = create_dataframe(SPARK, all_collateral_files)\n",
    "# print(\"Remove ND values.\")\n",
    "# tmp_df1 = replace_no_data(raw_collateral_df)\n",
    "# print(\"Replace Y/N with boolean flags.\")\n",
    "# tmp_df2 = replace_bool_data(tmp_df1)\n",
    "# print(\"Cast data to correct types.\")\n",
    "# final_df = cast_to_datatype(tmp_df2, run_props[\"COLLATERAL_COLUMNS\"])\n",
    "# (\n",
    "#     final_df.write\n",
    "#     .partitionBy(\"year\", \"month\", \"day\")\n",
    "#     .mode(\"append\")\n",
    "#     .parquet(\"../data/output/bronze/collaterals.parquet\")\n",
    "# )\n",
    "(\n",
    "    raw_collateral_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"../data/output/bronze/collaterals.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND INFO BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Bond_Info\"\n",
    "    # config[\"BOND_COLUMNS\"] = {\n",
    "    #     \"BS1\": DateType(),\n",
    "    #     \"BS2\": StringType(),\n",
    "    #     \"BS3\": DoubleType(),\n",
    "    #     \"BS4\": DoubleType(),\n",
    "    #     \"BS5\": BooleanType(),\n",
    "    #     \"BS6\": StringType(),\n",
    "    #     \"BS11\": DoubleType(),\n",
    "    #     \"BS12\": BooleanType(),\n",
    "    #     \"BS13\": DoubleType(),\n",
    "    #     \"BS19\": StringType(),\n",
    "    #     \"BS20\": StringType(),\n",
    "    # }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
    "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                    col_names[0] = \"BS1\"\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line)\n",
    "            df = (\n",
    "                spark.createDataFrame(content, col_names)\n",
    "                .withColumn(\"ed_code\", F.lit(csv_id))\n",
    "                .replace(\"\",None)\n",
    "                .withColumn(\"ImportDate\", F.lit(csv_date))\n",
    "                .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
    "                .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
    "                .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
    "                .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
    "                .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
    "                .withColumn(\"checksum\",F.md5(F.concat(F.col(\"BS1\"),F.col(\"BS2\"))))\n",
    "                .drop(\"ImportDate\")\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "# def replace_no_data(df):\n",
    "#     \"\"\"\n",
    "#     Replace ND values inside the dataframe\n",
    "#     TODO: ND are associated with labels that explain why the vaue is missing.\n",
    "#           Should handle this information better in future releases.\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :return df: Spark dataframe without ND values.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         df = df.withColumn(\n",
    "#             col_name,\n",
    "#             F.when(F.col(col_name).startswith(\"ND\"), None).otherwise(F.col(col_name)),\n",
    "#         )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def replace_bool_data(df):\n",
    "#     \"\"\"\n",
    "#     Replace Y/N with boolean flags in the dataframe.\n",
    "\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :return df: Spark dataframe without Y/N values.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         df = df.withColumn(\n",
    "#             col_name,\n",
    "#             F.when(F.col(col_name) == \"Y\", \"True\")\n",
    "#             .when(F.col(col_name) == \"N\", \"False\")\n",
    "#             .otherwise(F.col(col_name)),\n",
    "#         )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def cast_to_datatype(df, columns):\n",
    "#     \"\"\"\n",
    "#     Cast data to the respective datatype.\n",
    "\n",
    "#     :param df: Spark dataframe with loan asset data.\n",
    "#     :param columns: collection of column names and respective data types.\n",
    "#     :return df: Spark dataframe with correct values.\n",
    "#     \"\"\"\n",
    "#     for col_name, data_type in columns.items():\n",
    "#         if data_type == BooleanType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\"tmp_col_name\", F.col(col_name).contains(\"True\"))\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#         if data_type == DateType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\"tmp_col_name\", F.to_date(F.col(col_name)))\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#         if data_type == DoubleType():\n",
    "#             df = (\n",
    "#                 df.withColumn(\n",
    "#                     \"tmp_col_name\", F.round(F.col(col_name).cast(DoubleType()), 2)\n",
    "#                 )\n",
    "#                 .drop(col_name)\n",
    "#                 .withColumnRenamed(\"tmp_col_name\", col_name)\n",
    "#             )\n",
    "#     return df\n",
    "\n",
    "\n",
    "print(\"Start BOND INFO BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_bond_info_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "print(f\"Retrieved {len(all_bond_info_files)} bond info data files.\")\n",
    "raw_bond_info_df = create_dataframe(SPARK, all_bond_info_files)\n",
    "# print(\"Remove ND values.\")\n",
    "# tmp_df1 = replace_no_data(raw_bond_info_df)\n",
    "# print(\"Replace Y/N with boolean flags.\")\n",
    "# tmp_df2 = replace_bool_data(tmp_df1)\n",
    "# print(\"Cast data to correct types.\")\n",
    "# final_df = cast_to_datatype(tmp_df2, run_props[\"BOND_COLUMNS\"])\n",
    "# (\n",
    "#     final_df.write\n",
    "#     .partitionBy(\"year\", \"month\", \"day\")\n",
    "#     .mode(\"append\")\n",
    "#     .parquet(\"../data/output/bronze/bond_info.parquet\")\n",
    "# )\n",
    "(\n",
    "    raw_bond_info_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"../data/output/bronze/bond_info.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMORTISATION BRONZE\n",
    "\n",
    "To be reviewed since even one portfolio will bring it OutOfMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start AMORTISATION BRONZE job.\n",
      "Retrieved 19 amortisation data files.\n"
     ]
    }
   ],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Amortization\"\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.csv\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark dataframe for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
    "            csv_date =\"-\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4])\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                    col_names[0] = \"AS3\" #visible error from EDW where first columns has different names\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    content.append(line) # fix empty \"\" because they are messy\n",
    "            df = (\n",
    "                    spark.createDataFrame(content, col_names)\n",
    "                    .withColumn(\"ed_code\", F.lit(csv_id))\n",
    "                    .replace(\"\",None)\n",
    "                    .withColumn(\"ImportDate\", F.lit(csv_date))\n",
    "                    .withColumn(\"year\", F.year(F.col(\"ImportDate\")))\n",
    "                    .withColumn(\"month\", F.month(F.col(\"ImportDate\")))\n",
    "                    .withColumn(\"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType()))\\\n",
    "                    .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\\\n",
    "                    .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\\\n",
    "                    .withColumn(\"checksum\",F.md5(F.col(\"AS3\")))\n",
    "                    .drop(\"ImportDate\")\n",
    "                )\n",
    "            list_dfs.append(df)\n",
    "    return reduce(DataFrame.union, list_dfs)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start AMORTISATION BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_amortisation_files = get_raw_files(\n",
    "    run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"]\n",
    ")\n",
    "print(f\"Retrieved {len(all_amortisation_files)} amortisation data files.\")\n",
    "raw_amortisation_df = (\n",
    "            create_dataframe(SPARK, all_amortisation_files[:1])\n",
    "        )\n",
    "\n",
    "(\n",
    "    raw_amortisation_df.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"../data/output/bronze/amortisation.parquet\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEAL DETAILS BRONZE\n",
    "To be reviewed since it is unclear how to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"SOURCE_DIR\"] = BRONZE_SOURCE_DIR\n",
    "    config[\"FILE_KEY\"] = \"Deal_Details\"\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_raw_files(source_dir, file_key):\n",
    "    \"\"\"\n",
    "    Return list of files that satisfy the file_key parameter.\n",
    "    Works only on local machine so far.\n",
    "\n",
    "    :param source_dir: folder path where files are stored.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: listof desired files from source_dir.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in glob.glob(f\"{source_dir}/*/*{file_key}*.xml\")]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {source_dir}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def create_dataframe(deal_detail_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param deal_detail_files: files to be read to generate the dataframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    for xml_f in deal_detail_files:\n",
    "        xml_data = objectify.parse(xml_f)  # Parse XML data\n",
    "        root = xml_data.getroot()  # Root element\n",
    "\n",
    "        data = []\n",
    "        cols = []\n",
    "        for i in range(\n",
    "            len(\n",
    "                root.getchildren()[1]\n",
    "                .getchildren()[0]\n",
    "                .getchildren()[1]\n",
    "                .getchildren()[0]\n",
    "                .getchildren()\n",
    "            )\n",
    "        ):\n",
    "            child = (\n",
    "                root.getchildren()[1]\n",
    "                .getchildren()[0]\n",
    "                .getchildren()[1]\n",
    "                .getchildren()[0]\n",
    "                .getchildren()[i]\n",
    "            )\n",
    "            tag = child.tag.replace(\"{http://edwin.eurodw.eu/EDServices/2.3}\", \"\")\n",
    "            if tag == \"ISIN\":\n",
    "                # is array\n",
    "                data.append(\";\".join(map(str,child.getchildren())))\n",
    "            elif tag in [\"Country\", \"DealVisibleToOrg\", \"DealVisibleToUser\", \"Submissions\"]:\n",
    "                # usually null values\n",
    "                #TODO: Submissions might have interesting stuff. Ask to Luca.\n",
    "                continue\n",
    "            else:\n",
    "                data.append(child.text)\n",
    "            cols.append(tag)\n",
    "\n",
    "        df = pd.DataFrame(data).T  # Create DataFrame and transpose it\n",
    "        df.columns = cols  # Update column names\n",
    "        df[\"valid_from\"] = pd.Timestamp.now() \n",
    "        df[\"valid_to\"] = None\n",
    "        df[\"iscurrent\"] = 1\n",
    "        df[\"checksum\"] =pd.util.hash_pandas_object(df[\"EDCode\"])\n",
    "        list_dfs.append(df)\n",
    "    \n",
    "    return pd.concat(list_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Start DEAL DETAILS BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "all_xml_files = get_raw_files(run_props[\"SOURCE_DIR\"], run_props[\"FILE_KEY\"])\n",
    "raw_deal_details_df = create_dataframe(all_xml_files)\n",
    "\n",
    "# (\n",
    "#     SPARK.createDataFrame(raw_deal_details_df).write\n",
    "#     .mode(\"append\")\n",
    "#     .parquet(\"../data/output/bronze/deal_details.parquet\")\n",
    "# )\n",
    "(\n",
    "    raw_deal_details_df\n",
    "    .to_csv(\n",
    "        \"../data/output/bronze/deal_details.csv\", \n",
    "        mode='a', \n",
    "        header=not os.path.exists(\"../data/output/bronze/deal_details.csv\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
