{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "import csv\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.1\")\n",
    ")\n",
    "SPARK = builder.getOrCreate()\n",
    "\n",
    "BRONZE_SOURCE_DIR = \"../data/mini_source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"BUCKET_NAME\"] = \"../data\"\n",
    "    config[\"UPLOAD_PREFIX\"] = \"mini_source\"\n",
    "    config[\"BRONZE_PREFIX\"] = \"output/bronze/assets\"\n",
    "    config[\"FILE_KEY\"] = \"Loan_Data\"\n",
    "    config[\"SPARK\"] = SPARK\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_csv_files(bucket_name, prefix, file_key):\n",
    "    \"\"\"\n",
    "    Return list of CSV files that satisfy the file_key parameter from EDW.\n",
    "\n",
    "    :param bucket_name: GS bucket where files are stored.\n",
    "    :param prefix: specific bucket prefix from where to collect files.\n",
    "    :param file_key: label for file name that helps with the cherry picking.\n",
    "    :return all_files: list of desired files from source_dir.\n",
    "    \"\"\"\n",
    "    # storage_client = storage.Client(project=\"dataops-369610\")\n",
    "    # all_files = [\n",
    "    #     b.name\n",
    "    #     for b in storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    #     if (b.name.endswith(\".csv\"))\n",
    "    #     and (file_key in b.name)\n",
    "    #     and not (\"Labeled0M\" in b.name)\n",
    "    # ]\n",
    "    all_files = [f for f in glob.glob(f\"{bucket_name}/{prefix}/*/*Loan_Data*.csv\") if \"Labeled0M\" not in f]\n",
    "    if len(all_files) == 0:\n",
    "        print(\n",
    "            f\"No files with key {file_key.upper()} found in {bucket_name}. Exit process!\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def get_source_df(spark, bucket_name, prefix, pcds):\n",
    "    \"\"\"\n",
    "    Return BRONZE ASSET table, but only the partitions from the specified pcds.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param bucket_name: GS bucket where files are stored.\n",
    "    :param prefix: specific bucket prefix from where to collect files.\n",
    "    :params pcds: list of PCD from source files (valid only when generating TARGET dataframe).\n",
    "    :return df: Spark dataframe with the Asset information.\n",
    "    \"\"\"\n",
    "    # storage_client = storage.Client(project=\"dataops-369610\")\n",
    "    check_list = []\n",
    "    for pcd in pcds:\n",
    "        year = pcd.split(\"-\")[0]\n",
    "        month = pcd.split(\"-\")[1]\n",
    "        partition_prefix = f\"{prefix}/assets/year={year}/month={month}\"\n",
    "        check_list.append(\n",
    "            len(\n",
    "                # [\n",
    "                #     b.name\n",
    "                #     for b in storage_client.list_blobs(\n",
    "                #         bucket_name, prefix=partition_prefix\n",
    "                #     )\n",
    "                # ]\n",
    "                [\n",
    "                    f for f in glob.glob(f\"{bucket_name}/{prefix}/*.parquet\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    if sum(check_list) > 0:\n",
    "        df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .load(f\"gs://{bucket_name}/{prefix}\")\n",
    "            .withColumn(\"lookup\", F.concat_ws(\"-\", F.col(\"year\"), F.col(\"month\")))\n",
    "            .filter(F.col(\"lookup\").isin(pcds))\n",
    "            .drop(\"lookup\")\n",
    "        )\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_dataframe(spark, all_files):\n",
    "    \"\"\"\n",
    "    Read files and generate one PySpark DataFrame from them.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param all_files: list of files to be read to generate the dtaframe.\n",
    "    :return df: PySpark datafram for loan asset data.\n",
    "    \"\"\"\n",
    "    list_dfs = []\n",
    "    pcds = []\n",
    "    for csv_f in all_files:\n",
    "        col_names = []\n",
    "        content = []\n",
    "        with open(csv_f, \"r\") as f:\n",
    "            csv_id = csv_f.split(\"/\")[-1].split(\"_\")[0]\n",
    "            pcds.append(\"_\".join(csv_f.split(\"/\")[-1].split(\"_\")[1:4]))\n",
    "            for i, line in enumerate(csv.reader(f)):\n",
    "                if i == 0:\n",
    "                    col_names = line\n",
    "                    col_names[0] = \"AS1\"\n",
    "                elif i == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(line) == 0:\n",
    "                        continue\n",
    "                    content.append(line)\n",
    "            df = (\n",
    "                spark.createDataFrame(content, col_names)\n",
    "                .withColumn(\"ed_code\", F.lit(csv_id))\n",
    "                .replace(\"\", None)\n",
    "                .withColumn(\"year\", F.year(F.col(\"AS1\")))\n",
    "                .withColumn(\"month\", F.month(F.col(\"AS1\")))\n",
    "                .withColumn(\n",
    "                    \"valid_from\", F.lit(F.current_timestamp()).cast(TimestampType())\n",
    "                )\n",
    "                .withColumn(\"valid_to\", F.lit(\"\").cast(TimestampType()))\n",
    "                .withColumn(\"iscurrent\", F.lit(1).cast(\"int\"))\n",
    "                .withColumn(\n",
    "                    \"checksum\",\n",
    "                    F.md5(\n",
    "                        F.concat(\n",
    "                            F.col(\"ed_code\"),\n",
    "                            F.col(\"AS3\"),\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            list_dfs.append(df)\n",
    "    if list_dfs == []:\n",
    "        print(\"No dataframes were extracted from files. Exit process!\")\n",
    "        sys.exit(1)\n",
    "    return (pcds, reduce(DataFrame.union, list_dfs))\n",
    "\n",
    "\n",
    "def perform_scd2(spark, source_df, target_df):\n",
    "    \"\"\"\n",
    "    Perform SCD-2 to update legacy data at the bronze level tables.\n",
    "\n",
    "    :param source_df: Pyspark dataframe with data from most recent filset.\n",
    "    :param target_df: Pyspark dataframe with data from legacy filset.\n",
    "    :param spark: SparkSession object.\n",
    "    \"\"\"\n",
    "    source_df.createOrReplaceTempView(\"delta_table_asset\")\n",
    "    target_df.createOrReplaceTempView(\"staged_update\")\n",
    "    update_qry = \"\"\"\n",
    "        SELECT NULL AS mergeKey, source.*\n",
    "        FROM delta_table_asset AS target\n",
    "        INNER JOIN staged_update as source\n",
    "        ON target.id = source.id\n",
    "        WHERE target.checksum != source.checksum\n",
    "        AND target.iscurrent = 1\n",
    "    UNION\n",
    "        SELECT id AS mergeKey, *\n",
    "        FROM staged_update\n",
    "    \"\"\"\n",
    "    # Upsert\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO delta_table_asset tgt\n",
    "        USING ({update_qry}) src\n",
    "        ON tgt.id = src.mergeKey\n",
    "        WHEN MATCHED AND src.checksum != tgt.checksum AND tgt.iscurrent = 1 \n",
    "        THEN UPDATE SET valid_to = src.valid_from, iscurrent = 0\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "print(\"Start ASSETS BRONZE job.\")\n",
    "run_props = set_job_params()\n",
    "\n",
    "print(\"Create TARGET dataframe\")\n",
    "all_target_files = get_csv_files(\n",
    "    run_props[\"BUCKET_NAME\"],\n",
    "    run_props[\"UPLOAD_PREFIX\"],\n",
    "    run_props[\"FILE_KEY\"],\n",
    ")\n",
    "if len(all_target_files) == 0:\n",
    "    print(\"No new CSV files to retrieve. Workflow stopped!\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(f\"Retrieved {len(all_target_files)} asset data CSV files.\")\n",
    "    pcds, target_asset_df = create_dataframe(run_props[\"SPARK\"], all_target_files)\n",
    "\n",
    "    print(\"Retrieve SOURCE dataframe\")\n",
    "    source_asset_df = get_source_df(\n",
    "        run_props[\"SPARK\"],\n",
    "        run_props[\"BUCKET_NAME\"],\n",
    "        run_props[\"BRONZE_PREFIX\"],\n",
    "        pcds,\n",
    "    )\n",
    "    if source_asset_df is None:\n",
    "        print(\"Initial load into ASSET BRONZE\")\n",
    "        (\n",
    "            target_asset_df.write.partitionBy(\"year\", \"month\")\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .save(\n",
    "                f'gs://{run_props[\"BUCKET_NAME\"]}/{run_props[\"BRONZE_PREFIX\"]}/assets'\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Upsert data into ASSET BRONZE\")\n",
    "        perform_scd2(run_props[\"SPARK\"], source_asset_df, target_asset_df)\n",
    "\n",
    "print(\"End ASSETS BRONZE job.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:35:26) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
