{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, StringType, DoubleType, IntegerType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_job_params():\n",
    "    \"\"\"\n",
    "    Setup parameters used for this module.\n",
    "\n",
    "    :return config: dictionary with properties used in this job.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config[\"AMORTISATION_COLUMNS\"] = {\n",
    "        \"AS3\": StringType(),\n",
    "        \"pcd_year\": IntegerType(),\n",
    "        \"pcd_month\": IntegerType(),\n",
    "    }\n",
    "\n",
    "    for i in range(150, 270):\n",
    "        if i % 2 == 0:\n",
    "            config[\"AMORTISATION_COLUMNS\"][f\"AS{i}\"] = DoubleType()\n",
    "        else:\n",
    "            config[\"AMORTISATION_COLUMNS\"][f\"AS{i}\"] = DateType()\n",
    "    return config\n",
    "\n",
    "\n",
    "def _melt(df, id_vars, value_vars, var_name=\"FEATURE_NAME\", value_name=\"FEATURE_VALUE\"):\n",
    "    \"\"\"Convert DataFrame from wide to long format.\"\"\"\n",
    "    # Ref:https://stackoverflow.com/a/41673644\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(\n",
    "        *(\n",
    "            F.struct(F.lit(c).alias(var_name), F.col(c).alias(value_name))\n",
    "            for c in value_vars\n",
    "        )\n",
    "    )\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "    cols = id_vars + [\n",
    "        F.col(\"_vars_and_vals\")[x].cast(\"string\").alias(x)\n",
    "        for x in [var_name, value_name]\n",
    "    ]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n",
    "def unpivot_dataframe(df, columns):\n",
    "    \"\"\"\n",
    "    Convert dataframe from wide to long table.\n",
    "\n",
    "    :param df: raw Spark dataframe.\n",
    "    :param columns: data columns with respective datatype.\n",
    "    :return new_df: unpivot Spark dataframe.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "        \"AS3\", F.concat_ws(\"_\", F.col(\"AS3\"), F.monotonically_increasing_id())\n",
    "    )\n",
    "    date_columns = [\n",
    "        k for k, v in columns.items() if v == DateType() and k in df.columns\n",
    "    ]\n",
    "    double_columns = [\n",
    "        k for k, v in columns.items() if v == DoubleType() and k in df.columns\n",
    "    ]\n",
    "\n",
    "    date_df = _melt(\n",
    "        df,\n",
    "        id_vars=[\"AS3\"],\n",
    "        value_vars=date_columns,\n",
    "        var_name=\"DATE_COLUMNS\",\n",
    "        value_name=\"DATE_VALUE\",\n",
    "    ).filter(F.col(\"DATE_VALUE\").isNotNull())\n",
    "    double_df = _melt(\n",
    "        df,\n",
    "        id_vars=[\"AS3\"],\n",
    "        value_vars=double_columns,\n",
    "        var_name=\"DOUBLE_COLUMNS\",\n",
    "        value_name=\"DOUBLE_VALUE\",\n",
    "    ).filter(F.col(\"DOUBLE_VALUE\").isNotNull())\n",
    "\n",
    "    scd2_df = df.select(\"AS3\", \"part\", \"pcd_year\", \"pcd_month\")\n",
    "    new_df = (\n",
    "        date_df.join(double_df, on=\"AS3\", how=\"inner\")\n",
    "        .join(scd2_df, on=\"AS3\", how=\"inner\")\n",
    "        .withColumn(\"AS3\", F.split(F.col(\"AS3\"), \"_\").getItem(0))\n",
    "        .drop(\"DATE_COLUMNS\", \"DOUBLE_COLUMNS\")\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+---------+--------+---------+\n",
      "|    AS3|DATE_VALUE|DOUBLE_VALUE|     part|pcd_year|pcd_month|\n",
      "+-------+----------+------------+---------+--------+---------+\n",
      "|5301203|2014-10-27|    18134.99|SME_20147|    2014|        7|\n",
      "|5301203|2014-10-27|    36382.99|SME_20147|    2014|        7|\n",
      "|5301203|2014-11-27|    10798.06|SME_20147|    2014|        7|\n",
      "|5301203|2015-01-27|    13256.18|SME_20147|    2014|        7|\n",
      "|5301203|2015-07-27|    50174.75|SME_20147|    2014|        7|\n",
      "|5301203|2016-06-27|     15701.8|SME_20147|    2014|        7|\n",
      "|5301203|2016-11-27|    37936.33|SME_20147|    2014|        7|\n",
      "|5301203|2016-11-27|    38711.02|SME_20147|    2014|        7|\n",
      "|5301203|2017-01-27|    33260.44|SME_20147|    2014|        7|\n",
      "|5301203|2017-03-27|    20555.81|SME_20147|    2014|        7|\n",
      "|5301203|2017-11-27|    44861.36|SME_20147|    2014|        7|\n",
      "|5301203|2018-01-27|     6673.24|SME_20147|    2014|        7|\n",
      "|5301203|2018-02-27|    20555.81|SME_20147|    2014|        7|\n",
      "|5301203|2018-03-27|    30904.56|SME_20147|    2014|        7|\n",
      "|5301203|2018-03-27|    37160.32|SME_20147|    2014|        7|\n",
      "|5301203|2018-04-27|     5013.47|SME_20147|    2014|        7|\n",
      "|5301203|2018-04-27|    32476.48|SME_20147|    2014|        7|\n",
      "|5301203|2019-06-27|    20555.81|SME_20147|    2014|        7|\n",
      "|5301203|2019-06-27|     43331.6|SME_20147|    2014|        7|\n",
      "|5300210|2014-09-27|    25779.61|SME_20147|    2014|        7|\n",
      "+-------+----------+------------+---------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pcd = \"2014_10\"\n",
    "run_props = set_job_params()\n",
    "# part_pcd = pcd.replace(\"_0\", \"\").replace(\"_\", \"\")\n",
    "bronze_df = (spark.read.parquet(\"../data/amortisation_bronze\")\n",
    "             .where(F.col(\"part\") == \"SME_20147\")\n",
    "             .filter(F.col(\"iscurrent\") == 1)\n",
    "            .drop(\"valid_from\", \"valid_to\", \"checksum\", \"iscurrent\"))\n",
    "tmp_df = unpivot_dataframe(bronze_df, run_props[\"AMORTISATION_COLUMNS\"])\n",
    "info_df = tmp_df.withColumn(\n",
    "    \"DATE_VALUE\", F.to_date(F.col(\"DATE_VALUE\"))\n",
    ").withColumn(\n",
    "    \"DOUBLE_VALUE\", F.round(F.col(\"DOUBLE_VALUE\").cast(DoubleType()), 2)\n",
    ")\n",
    "\n",
    "(\n",
    "    info_df.write.format(\"parquet\")\n",
    "    .partitionBy(\"pcd_year\", \"pcd_month\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"../data/amortisation_silver\")\n",
    ")\n",
    "info_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
